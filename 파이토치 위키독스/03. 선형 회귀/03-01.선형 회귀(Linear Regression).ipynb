{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049406f8-9c85-449d-9482-3c540dd993cc",
   "metadata": {},
   "source": [
    "### 01. 선형 회귀(Linear Regression)\n",
    "- 선형 회귀 이론에 대해서 공부하고 파이토치를 이용해여 선형 회귀 모델 만들기\n",
    "    - 데이터에 대한 이해 : 학습할 데이터에 대해서 공부\n",
    "    - 가설 수립 : 가설을 수립하는 방법에 대해서 공부\n",
    "    - 손실 계산하기 : 학습 데이터를 이용해서 연속적으로 모델을 개선시키는데 이 때 손실을 이용\n",
    "    - 경사 하강법 : 학습을 위한 핵심 알고리즘인 경사하강법에 대해서 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733063fa-7a69-4e01-8c38-158e342c545f",
   "metadata": {},
   "source": [
    "#### 1. 데이터에 대한 이해(Data Definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89bfd31-dfe5-4047-84af-7eb5ecd75457",
   "metadata": {},
   "source": [
    "##### 1. 훈련 데이터셋과 테스트 데이터셋\n",
    "- 어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간 공부했더니 6점을 맞았다면 내가 4시간 공부 한다면 몇 점을 맞을 수 있는가 ?\n",
    "- 질문에 대답하기 위해서 1시간, 2시간, 3시간을 공부했을 때 각각 2점, 4점, 6점이 나왔다는 앞서 나온 정보를 이용해야 한다.\n",
    "- 이때 예측을 위해 사용하는 데이터를 훈련 데이터셋이라고 한다.\n",
    "- 학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하는 데이터셋을 테스트 데이터셋이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b69fca6-4dd8-4160-b19c-e806cc872c66",
   "metadata": {},
   "source": [
    "##### 2. 훈련 데이터셋의 구성\n",
    "- 앞서 텐서에 대해서 배웠는데 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 한다.\n",
    "- 입력과 출력을 각각 다른 텐서에 저장할 필요가 있다.\n",
    "- 보편적으로 입력은 x, 출력은 y를 사용하여 표기한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c312cd5-c08b-433d-921f-a64999bee767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train : 공부시간, y_train : 점수\n",
    "import torch\n",
    "import numpy as np\n",
    "x_train = torch.FloatTensor([[1] ,[2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc2720-bdae-4658-973b-7c887e16b51f",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a392273-fe78-4517-882c-3803d0d3c2e7",
   "metadata": {},
   "source": [
    "#### 2. 가설(Hypothesis)수립\n",
    "- 머신러닝에서 식을 세울 때 이 식을 가설이라고 한다\n",
    "    - 보통 머신러닝에서 가설은 임의로 추측해서 세워보는 식\n",
    "    - 경험적으로 알고 있는 식\n",
    "    - 맞는 가설이 아니라고 판단되면 계속 수정해나가게 되는 식\n",
    "- 선형회귀의 가설은 널리 알려져 있다. \n",
    "     - 선형회귀 : 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일\n",
    "     - 선형 회귀의 가설(직선의 방정식) : y = Wx + b or H(x) = Wx + b\n",
    "     - W : 가중치(weight), b : 편향(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff2047-9c2e-46c9-bb37-74edaef17683",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d9aa1-c1e5-455a-aa77-8ef71b372493",
   "metadata": {},
   "source": [
    "#### 3. 비용함수(Cost function)에 대한 이해\n",
    "- 비용함수(cost function) = 손실함수(loss function) = 오차함수(error function) = 목적함수(objective function)\n",
    "- ex) 그래프 위의 4개의 점에서 이 점들을 가장 잘 나타내는 직선을 찾고 싶을 때, 4개의 직선이 후보로 주어진다면 오차가 가장 작은 직선을 선택해야 한다. \n",
    "- 방법 : MSE(평균 제곱 오차) : 회귀 문제에서 적절한 W와 b를 찾기위해서 최적화된 식 \n",
    "- 즉, 평균 제곱 오차를 W와 b에 의한 비용함수로 재정의하여 비용함수를 최소가 되게 만드는 W와 b를 구하면 훈련 데이터를 가장 잘 나타내는 직선을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bc04e-3b5d-403a-afaa-cef17d7f1f85",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f3297-9906-4ba5-a4ce-3d2bbb989409",
   "metadata": {},
   "source": [
    "#### 4. 옵티마이저 - 경사하강법(Gradient Descent)\n",
    "- 앞서 정의한 비용함수의 값을 최소화하는 W와 b를 찾는 방법에 대해서 배울 차례로 이때 사용되는 것이 옵티마이저 알고리즘(최적화 알고리즘이라고 부르기도 한다.)\n",
    "- 이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부른다.\n",
    "- 가장 기본적인 옵티마이저 알고리즘인 경사하강법에 대해서 배운다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02623ccc-e70e-4188-96b5-f2bd68f9f931",
   "metadata": {},
   "source": [
    "##### [예제]\n",
    "- 이 부분에서는 편향 b=0으로 가정하고 y=Wx와 같은 식을 기준으로 설명\n",
    "    - 기울기(W)의 값이 지나치게 크거나 작으면 실제값과 예측값의 오차가 커진다. \n",
    "    - b 또한 지나치게 크거나 작으면 오차가 커진다. \n",
    "- 비용함수의 값을 cost라고 표현할 때, W와 cost의 관계는 2차곡선과 같다.\n",
    "    - W가 무한대로 커지거나 작아지면 cost의 값은 무한대로 커진다.\n",
    "    - cost가 가장 작을 때는 맨 아래의 볼록한 부분 -> 이 부분의 W 값을 찾아야 한다.\n",
    "    1) 기계는 임의의 초기값 W값을 정한 뒤에 맨 아래의 볼록한 부분을 향해 점차 W의 값을 수정해나간다. -> 경사하강법 이용(한점에서의 순간 변화율 또는 접선에서의 기울기의 개념을 사용)\n",
    "    2) cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며 미분값이 0이 되는 지점이다.\n",
    "- 경사 하강법의 아이디어 : 비용함수를 미분하여 현재 W에서의 접선의 기울기를 구하고 접선의 기울기가 가장 낮은 방향으로 w의 값을 변경하는 작업을 반복 -> 현재 W에 접선의 기울기를 구해 특정 숫자 알파를 곱한 값을 빼서 새로운 W로 사용하는 식이 사용\n",
    "    - 기울기가 음수일 때 : W의 값이 증가 -> W:= W - 알파x(음수기울기) = w + 알파x(양수기울기) -> 0인 방향으로 조정\n",
    "    - 기울기가 양수일 때 : W의 값이 감소 -> W:= W - 알파x(양수기울기) -> 0인 방향으로 조정\n",
    "- 학습률(learning rate)\n",
    "    - 학습률(알파)은 W의 값을 변경할 때, 얼마나 크게 변경할지를 결정\n",
    "    - W를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정\n",
    "    - 학습률 알파가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 W를 찾아가는 것이 아니라 W의 값이 발산하고 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 알파의 값을 찾아내는 것도 중요하다.\n",
    "- 위의 예제에서는 b는 배제시키고 최적의 W를 찾아내는 것에만 초점을 맞추어 경사 하강법의 원리에 대해 공부했는데, 실제로는 W와 b에 대해서 동시에 경사 하강법을 수행하면서 최적의 W와 b의 값을 찾아간다.\n",
    "\n",
    "- 가설, 비용함수, 옵티마이저는 머신러닝 분야에서 사용되는 포괄적인 개념이다. 풀고자하는 각 문제에 따라 가설, 비용함수, 옵티마이저는 전부 다를 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beae0cd-5968-4bb9-90ab-033aaf599319",
   "metadata": {},
   "source": [
    "**선형 회귀에 가장 적합한 비용함수는 [평균제곱오차], 옵티마이저는 [경사하강법]!!이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1b4a1-c1a6-4104-8284-f8a45ab10982",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb748389-0cb5-4f4c-ba92-d69f30087de2",
   "metadata": {},
   "source": [
    "#### 5. 파이토치로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f4152-ce1c-4022-a3a4-2c6f988059d8",
   "metadata": {},
   "source": [
    "##### 1. 기본 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299abd2c-2cc5-427e-a608-dc9fdc3fe1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964e3a0c-2e53-4432-a161-ab4d6c32e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3370170830>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코들 재실행해도 다음과 같은 결과가 나오도록 랜덤 시드를 준다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452421c-8c62-4f72-8877-72b25360f446",
   "metadata": {},
   "source": [
    "##### 2. 변수선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6030f73-e499-49f9-848d-421bee9c8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b022994-7e71-4c33-a0a7-f343dd05708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 각각의 크기(shape) 출력\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d82712-0c5a-4ecb-ba1d-773f76d299d5",
   "metadata": {},
   "source": [
    "##### 3. 가중치와 편향의 초기화\n",
    "- 선형회귀 : 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일\n",
    "- 가장 잘 맞는 직선을 정의하는 것 : W와 b\n",
    "- 선형 회귀의 목표 : 가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6e225-6b61-460f-9eed-a922401ea3fc",
   "metadata": {},
   "source": [
    "가중치 W를 0으로 초기화, 값을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c3e364-b2c3-4c38-a55c-c1a3009b6e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화\n",
    "# 학습을 통해 값이 변경되는 변수임을 명시\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "# 가중치 W를 출력\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba6d6e3-769e-4c0f-b8d6-ef1c1ae9e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 편향 b도 0으로 초기화\n",
    "# 학습을 통해 값이 변경되는 변수임을 명시\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cd56b-7529-4e44-8fa9-061723ecdb4f",
   "metadata": {},
   "source": [
    "현재 가중치 W와 b 둘다 0이므로 현 직선의 방정식 : y = 0Xx + 0\n",
    "- 지금 상태에선 x에 어떤 값이 들어가도 가설은 0을 예측하게 된다.\n",
    "- 즉, 아직은 적절한 W와 b의 값이 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df791bae-1746-4967-9c1b-e0d712ee956e",
   "metadata": {},
   "source": [
    "##### 4. 가설 세우기\n",
    "파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언 : H(x) = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a629d876-1972-4631-ba39-a474055ad74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b4328-8c9e-47e5-9e87-153eff982a7e",
   "metadata": {},
   "source": [
    "##### 5. 비용함수 선언하기\n",
    "파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c8172d-5c76-4547-8e7d-c3dac8316945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) **2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd1b159-d0f2-4a3c-85b0-8bd342be18ab",
   "metadata": {},
   "source": [
    "##### 6. 경사 하강법 구현하기\n",
    "- 'SGD' : 경사 하강법의 일종\n",
    "- lr : 학습률\n",
    "- 학습 대상인 W와 b가 SGD의 입력이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cf6d7ec-fbc2-48f2-9018-d4f1c0ae505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d940e-d549-432f-b8f4-d55bf7ce4deb",
   "metadata": {},
   "source": [
    "- optimize.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화한다. \n",
    "    - 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있다.\n",
    "- 그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산된다.\n",
    "- 그 다음 경사 하강법 최적화 함수 optimizer의 .step() 함수를 호출하여 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률 0.01을 곱하여 빼줌으로써 업데이트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198edd30-eb2f-4d3a-9855-c3156fd1d6f3",
   "metadata": {},
   "source": [
    "- requires_grad = True와 backward()에 대한 정리는 자동미분 챕터에 다시 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7202f35-6603-4ecc-8ed1-324df4ce17d3",
   "metadata": {},
   "source": [
    "##### 7. 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f0f6b8-3f94-48f1-a6c9-703c2bf5b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W : 0.186667, b : 0.080 Cost : 18.666666\n",
      "Epoch  100/2000 W : 1.745691, b : 0.578 Cost : 0.048171\n",
      "Epoch  200/2000 W : 1.800099, b : 0.454 Cost : 0.029767\n",
      "Epoch  300/2000 W : 1.842860, b : 0.357 Cost : 0.018394\n",
      "Epoch  400/2000 W : 1.876473, b : 0.281 Cost : 0.011366\n",
      "Epoch  500/2000 W : 1.902897, b : 0.221 Cost : 0.007024\n",
      "Epoch  600/2000 W : 1.923668, b : 0.174 Cost : 0.004340\n",
      "Epoch  700/2000 W : 1.939996, b : 0.136 Cost : 0.002682\n",
      "Epoch  800/2000 W : 1.952832, b : 0.107 Cost : 0.001657\n",
      "Epoch  900/2000 W : 1.962921, b : 0.084 Cost : 0.001024\n",
      "Epoch 1000/2000 W : 1.970853, b : 0.066 Cost : 0.000633\n",
      "Epoch 1100/2000 W : 1.977087, b : 0.052 Cost : 0.000391\n",
      "Epoch 1200/2000 W : 1.981989, b : 0.041 Cost : 0.000242\n",
      "Epoch 1300/2000 W : 1.985842, b : 0.032 Cost : 0.000149\n",
      "Epoch 1400/2000 W : 1.988870, b : 0.025 Cost : 0.000092\n",
      "Epoch 1500/2000 W : 1.991251, b : 0.020 Cost : 0.000057\n",
      "Epoch 1600/2000 W : 1.993122, b : 0.016 Cost : 0.000035\n",
      "Epoch 1700/2000 W : 1.994594, b : 0.012 Cost : 0.000022\n",
      "Epoch 1800/2000 W : 1.995750, b : 0.010 Cost : 0.000013\n",
      "Epoch 1900/2000 W : 1.996659, b : 0.008 Cost : 0.000008\n",
      "Epoch 2000/2000 W : 1.997374, b : 0.006 Cost : 0.000005\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1) : \n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) **2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0 : \n",
    "        print('Epoch {:4d}/{} W : {:3f}, b : {:.3f} Cost : {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179f5ac-d407-485a-afc6-aced350c599a",
   "metadata": {},
   "source": [
    "- 최종 훈련 결과를 보면 최적의 기울기 W는 2에 가깝고, b는 0에 가깝다.\n",
    "- 현재 훈련 데이터가 x_train은 [[1],[2],[3]]이고 y_train은 [[2],[4],[6]]인 것을 감안하면 실제 정답은 W가 2이고, b가 0인 H(x)=2x이므로 거의 정답을 찾은 셈이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acd090-98a3-4da4-b372-2280ce96de18",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0bcd6-119c-44d4-ba03-397c1c304b3e",
   "metadata": {},
   "source": [
    "#### 5. optimizer.zero_grad()가 필요한 이유\n",
    "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe9bbdf1-8862-4faf-b59c-8e5f2537033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n",
      "수식을 w로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1) : \n",
    "    \n",
    "    z = 2*w\n",
    "    \n",
    "    z.backward()\n",
    "    print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba8d1a-b9dd-4047-8178-eea075c66817",
   "metadata": {},
   "source": [
    "계속해서 미분값이 2가 누적되므로 optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화시켜주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56470c8-53ab-4758-8017-92faf81ec97a",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03c194-c02e-4f39-948f-718ffc95dbbd",
   "metadata": {},
   "source": [
    "#### 6. torch.manual_seed()를 하는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14194a75-03fb-415c-81d6-d2944c53177e",
   "metadata": {},
   "source": [
    "- torch.manual_seed()를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있다.\n",
    "- 이유 : torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해준다는 특징 때문\n",
    "    1. 랜덤 시드가 3일 때 두번 난수를 발생시켜보고\n",
    "    2. 다른 랜덤 시드를 사용한 후에 다시 랜덤 시드를 3을 사용한다면 난수 발생값이 동일하게 나오는지 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b761f89d-df1a-488e-a99a-6d7df835bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1,3) : \n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2e4cbeb-49fd-4ae7-b9af-643c8cffd93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 5일 때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1,3) : \n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04137cf5-7b09-4fcd-a340-b22b23195332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 다시 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 다시 3일 때')\n",
    "for i in range(1,3) : \n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f6918-2456-434b-8ff5-816e8113e27e",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c77cb-43c9-40f9-8c3a-f6d1585cf60d",
   "metadata": {},
   "source": [
    "- 텐서에는 requires_grad라는 속성이 있다. 이것을 True로 설정하면 자동 미분 기능이 적용된다.\n",
    "- 선형 회귀부터 신경망과 같은 복잡한 구조에서 파라미터들이 모두 이 기능이 적용된다.\n",
    "- requires_grad = True가 적용된 텐서에 연산을 하면, 계산 그래프가 생성되며 backward 함수를 호출하면 그래프로부터 자동으로 미분이 계산된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
