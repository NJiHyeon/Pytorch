{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 데이터 증식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `transf`\n",
    "    - Dataset을 정의하기 전에 정의\n",
    "    - 많은 기능들이 이미지 타입이 PIL일 경우 작동하기 때문에 사용전에 데이터 타입을 확인해야 한다.\n",
    "- 그외\n",
    "    - 회전, 흑백 이미지 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tr\n",
    "import PIL\n",
    "\n",
    "transf = tr.Compose([tr.ToPILImage(), \n",
    "                     tr.RandomCrop(60),\n",
    "                     # 이미지 밝기, 대비, 색조를 변형\n",
    "                     tr.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                     # 이미지 뒤집기\n",
    "                     tr.RandomHorizontalFlip(), \n",
    "                     tr.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 학습, 검증, 평가 데이터 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(dataset, [30000, 20000])\n",
    "trainloader =  torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 GPU 연산 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is available.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{device} is available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module) :\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1) :\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # 컨볼루션 연산 2개를 포함한 블록 만들기\n",
    "            # Batch Normalization : 각 배치의 평균과 분산을 이용해서 데이터를 정규화하는 방법으로 학습을 빠르게 할 수 있다.\n",
    "        self.conv_block = nn.Sequential(nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                                        nn.BatchNorm2d(self.out_channels),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                        nn.BatchNorm2d(self.out_channels))\n",
    "        \n",
    "        # 위의 설명 참고 (사이즈가 달라지거나, 채널이 달라지면 ~ 조건문 실행)\n",
    "        if self.stride != 1 or self.in_channels != self.out_channels :\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                                            nn.BatchNorm2d(self.out_channels))\n",
    "            \n",
    "    def forward(self, x) :\n",
    "        out = self.conv_block(x)\n",
    "        \n",
    "        # 사이즈 조정이 필요하다면 입력값의 크기를 조정한다. \n",
    "        if self.stride != 1 or self.in_channels != self.out_channels :\n",
    "            x = self.downsample(x)\n",
    "            \n",
    "        # Skip connection\n",
    "        out = F.relu(x + out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class ResNet(nn.Module) :\n",
    "    def __init__(self, num_blocks, num_classes=10) :\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # 입력 이미지가 들어와 연산을 수행하는 기본층\n",
    "        self.base = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.BatchNorm2d(64),\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        # 기본층을 제외한 4개의 블록이 필요하므로, self._make_layer를 이용하여 4개의 블록 묶음을 선언한다. \n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # 합성곱 층들을 지나면 최종적으로 크기가 4x4인 feature map 512개가 나온다. \n",
    "        # 크기가 4x4인 평균 풀링을 이용하면 각 피쳐맵 당 1개의 평균값이 나오기 때문에 성분이 512개인 벡터를 얻을 수 있다.\n",
    "        self.gap = nn.AvgPool2d(4) # 4 is filter size\n",
    "        \n",
    "        # 클래스가 10개인 이미지를 분류하는 것이므로 최종적으로 512개의 노드에서 10개의 노드로 가는 FC를 정의한다. \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        \n",
    "    def _make_layer(self, out_channels, num_blocks, stride) :\n",
    "        # 블록의 반복 횟수만큼 stride를 저장\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        \n",
    "        # ResidualBlock을 불러와서 append를 이용해 차례로 붙여준다.\n",
    "        # 이때 이전 출력 채널 크기와 다음 입력 채널 크기가 같아야 한다. \n",
    "        layers = []\n",
    "        for stride in strides :\n",
    "            block = ResidualBlock(self.in_channels, out_channels, stride)\n",
    "            layers.append(block) \n",
    "            self.in_channels = out_channels\n",
    "            \n",
    "        # nn.Sequential에 넣어 모델을 구축한다.\n",
    "        # *리스트 : 리스트의 길이에 상관 없이 모든 성분을 별도로 nn.Sequential에 전달하는 역할을 한다. \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    # ResNet 클래스 안에 연산을 행하는 forward\n",
    "    def forward(self, x) :\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.gap(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스를 불러오는 함수\n",
    "# 각 모델마다 블록의 반복 횟수를 리스트로 정의하여 입력한다. \n",
    "# 이를 통해 18, 34 외에 4개의 블록을 가진 레이어의 개수를 조정할 수 있다. \n",
    "def modeltype(model) :\n",
    "    if model == 'resnet18' :\n",
    "        return ResNet([2, 2, 2, 2])\n",
    "    elif model == 'resnet34' :\n",
    "        return ResNet([3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = modeltype('resnet18').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
      "     ResidualBlock-9           [-1, 64, 32, 32]               0\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-14           [-1, 64, 32, 32]             128\n",
      "    ResidualBlock-15           [-1, 64, 32, 32]               0\n",
      "           Conv2d-16          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-17          [-1, 128, 16, 16]             256\n",
      "             ReLU-18          [-1, 128, 16, 16]               0\n",
      "           Conv2d-19          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
      "           Conv2d-21          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
      "    ResidualBlock-23          [-1, 128, 16, 16]               0\n",
      "           Conv2d-24          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
      "             ReLU-26          [-1, 128, 16, 16]               0\n",
      "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
      "    ResidualBlock-29          [-1, 128, 16, 16]               0\n",
      "           Conv2d-30            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-31            [-1, 256, 8, 8]             512\n",
      "             ReLU-32            [-1, 256, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "           Conv2d-35            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "    ResidualBlock-37            [-1, 256, 8, 8]               0\n",
      "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
      "             ReLU-40            [-1, 256, 8, 8]               0\n",
      "           Conv2d-41            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-42            [-1, 256, 8, 8]             512\n",
      "    ResidualBlock-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-46            [-1, 512, 4, 4]               0\n",
      "           Conv2d-47            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-48            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-49            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-50            [-1, 512, 4, 4]           1,024\n",
      "    ResidualBlock-51            [-1, 512, 4, 4]               0\n",
      "           Conv2d-52            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-53            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-54            [-1, 512, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "    ResidualBlock-57            [-1, 512, 4, 4]               0\n",
      "        AvgPool2d-58            [-1, 512, 1, 1]               0\n",
      "           Linear-59                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 13.63\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 56.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델 출력\n",
    "torchsummary.summary(resnet, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 손실 함수 및 최적화 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_resnet_early.pth'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 검증 데이터에 대한 손실 함수값을 연산하는 함수 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가만 하기 때문에 **requires_grad**를 비활성화\n",
    "- 평가 시 **정규화 기법**들이 작동하지 않도록 eval 모드로 설정\n",
    "- 다시 모델을 train 모드로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(dataloader) :\n",
    "    n = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad() :\n",
    "        resnet.eval()\n",
    "        for data in dataloader : \n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = resnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    resnet.train()\n",
    "    return running_loss/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 그래프를 그리기 위해\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# 매 에폭마다 평균 손실함수값을 구하기 위해 n 설정\n",
    "n = len(trainloader)\n",
    "\n",
    "# 가장 낮은 검증 손실 함수값에 해당하는 모델을 저장하기 위해 손실 함수값 초기 기준을 1로 설정\n",
    "early_stopping_loss = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d48a4aa75719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5) :\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader :\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    train_loss = running_loss / n\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss = validation_loss(valloader)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('[%d] train loss : %.3f, validation loss : %.3f' % (epoch+1, train_loss, val_loss))\n",
    "    \n",
    "    if val_loss < early_stopping_loss :\n",
    "        torch.save(resnet.state_dict(), PATH)\n",
    "        ealry_stopping_train_loss = train_loss\n",
    "        early_stopping_val_loss = val_loss\n",
    "        early_stopping_epoch = epoch\n",
    "        \n",
    "print('Fianl pretrained model >> [%d] train loss : %.3f, validation loss : %.3f' % (early_stopping_epoch+1, early_stopping_train_loss, early_stopping_val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 손실 함수값 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAW8ElEQVR4nO3df5BV9Znn8fcTRBHRgICmBQ2YUEYgBEiHUKsxuqgF+DMJm5DRjDqVUJpY0UxSkdHMmkxtqswk61DWGhkza0orTBxHY3RTGKMWJlqJRnAQRTSg0aVtVGRXxSDrj3n2jz4wTec23c293Zfm+35V3ep7vj/Ofb7coj/3nHv73MhMJEnlek+zC5AkNZdBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEi7ERHPRcTJza5D6k8GgSQVziCQ+igiDoiIJRHRXt2WRMQBVd+YiPhFRLwaEf8nIh6IiPdUfZdFxAsRsTUino6IOc1didRhv2YXIA1CVwCzgelAAncA3wL+Fvg60AaMrcbOBjIijgEuBj6Wme0RMQEYMrBlS7V5RCD13TnA32Xmy5m5GfgO8IWq722gBXh/Zr6dmQ9kxwW93gUOACZHxNDMfC4zn2lK9VIXBoHUd0cAz3fafr5qA/g+sAH4VUQ8GxGLATJzA3Ap8G3g5Yi4OSKOQNoLGARS37UD7++0fVTVRmZuzcyvZ+bRwBnAX+94LyAz/zkzj6/mJvC9gS1bqs0gkHo2NCKG7bgBPwW+FRFjI2IM8F+BnwBExOkR8cGICOB1Ok4JvRsRx0TEf67eVN4OvFn1SU1nEEg9W07HL+4dt2HASmAN8DjwKPDfqrGTgHuBN4DfAT/MzPvpeH/gKuAV4EXgMODyAVuBtBvhF9NIUtk8IpCkwhkEklQ4g0CSCmcQSFLhBuUlJsaMGZMTJkxodhmSNKisWrXqlcwc27V9UAbBhAkTWLlyZbPLkKRBJSKer9XuqSFJKpxBIEmFMwgkqXCD8j0CSfuOt99+m7a2NrZv397sUvYZw4YNY/z48QwdOrRX4w0CSU3V1tbGwQcfzIQJE+i4Vp/qkZls2bKFtrY2Jk6c2Ks5nhqS1FTbt29n9OjRhkCDRASjR4/u0xGWQSCp6QyBxurrv6dBIEmFMwgkFe/VV1/lhz/8YZ/nzZ8/n1dffbXxBQ0wg0BS8boLgnff3f2XyC1fvpyRI0f2U1UDx08NSSre4sWLeeaZZ5g+fTpDhw5lxIgRtLS0sHr1ap588knOPvtsNm7cyPbt27nkkktYtGgR8B+Xu3njjTeYN28exx9/PL/97W8ZN24cd9xxBwceeGCTV9Y7BoGkvcZ3/tdanmx/vaH7nHzEIVx5xpTdjrnqqqt44oknWL16Nffffz+nnXYaTzzxxM6PX95www0ceuihvPnmm3zsYx/jM5/5DKNHj95lH+vXr+enP/0pP/rRj/jsZz/LbbfdxrnnntvQtfQXg0CSupg1a9Yun8G/5ppruP322wHYuHEj69ev/7MgmDhxItOnTwfgox/9KM8999xAlVs3g0DSXqOnV+4D5aCDDtp5//777+fee+/ld7/7HcOHD+fEE0+s+Rn9Aw44YOf9IUOG8Oabbw5IrY3gm8WSinfwwQezdevWmn2vvfYao0aNYvjw4Tz11FM89NBDA1xd//OIQFLxRo8ezXHHHcfUqVM58MADOfzww3f2zZ07l6VLlzJt2jSOOeYYZs+e3cRK+0dkZrNr6LPW1tb0i2mkfcO6des49thjm13GPqfWv2tErMrM1q5jPTUkSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFIfjRgxAoD29nYWLFhQc8yJJ55ITx9zX7JkCdu2bdu53azLWhsEkrSHjjjiCG699dY9nt81CJp1WeuGBEFEzI2IpyNiQ0QsrtEfEXFN1b8mImZ26R8SEf8WEb9oRD2S1BeXXXbZLt9H8O1vf5vvfOc7zJkzh5kzZ/LhD3+YO+6448/mPffcc0ydOhWAN998k4ULFzJt2jQ+97nP7XKtoYsuuojW1lamTJnClVdeCXRcyK69vZ2TTjqJk046Cei4rPUrr7wCwNVXX83UqVOZOnUqS5Ys2fl4xx57LF/60peYMmUKp556akOuaVT3JSYiYghwLXAK0AY8EhF3ZuaTnYbNAyZVt48D11U/d7gEWAccUm89kgaxuxbDi483dp/v+zDMu2q3QxYuXMill17Kl7/8ZQBuueUWfvnLX/K1r32NQw45hFdeeYXZs2dz5plndvt9wNdddx3Dhw9nzZo1rFmzhpkz/+P17ne/+10OPfRQ3n33XebMmcOaNWv46le/ytVXX82KFSsYM2bMLvtatWoVP/7xj3n44YfJTD7+8Y/zyU9+klGjRvXL5a4bcUQwC9iQmc9m5lvAzcBZXcacBdyUHR4CRkZEC0BEjAdOA/6pAbVIUp/NmDGDl19+mfb2dh577DFGjRpFS0sLl19+OdOmTePkk0/mhRde4KWXXup2H7/5zW92/kKeNm0a06ZN29l3yy23MHPmTGbMmMHatWt58sknu9sNAA8++CCf+tSnOOiggxgxYgSf/vSneeCBB4D+udx1Iy46Nw7Y2Gm7jV1f7Xc3ZhywCVgCfBM4eHcPEhGLgEUARx11VF0FS9pL9fDKvT8tWLCAW2+9lRdffJGFCxeybNkyNm/ezKpVqxg6dCgTJkyoefnpzmodLfzxj3/kBz/4AY888gijRo3i/PPP73E/u7sGXH9c7roRRwS1jpO6rqLmmIg4HXg5M1f19CCZeX1mtmZm69ixY/ekTknq1sKFC7n55pu59dZbWbBgAa+99hqHHXYYQ4cOZcWKFTz//PO7nX/CCSewbNkyAJ544gnWrFkDwOuvv85BBx3Ee9/7Xl566SXuuuuunXO6u/z1CSecwM9//nO2bdvGn/70J26//XY+8YlPNHC1u2rEEUEbcGSn7fFAey/HLADOjIj5wDDgkIj4SWYOju93k7TPmDJlClu3bmXcuHG0tLRwzjnncMYZZ9Da2sr06dP50Ic+tNv5F110ERdccAHTpk1j+vTpzJo1C4CPfOQjzJgxgylTpnD00Udz3HHH7ZyzaNEi5s2bR0tLCytWrNjZPnPmTM4///yd+/jiF7/IjBkz+u1bz+q+DHVE7Af8AZgDvAA8AvxFZq7tNOY04GJgPh2nja7JzFld9nMi8I3MPL2nx/Qy1NK+w8tQ94++XIa67iOCzHwnIi4G7gaGADdk5tqIuLDqXwospyMENgDbgAvqfVxJUmM05BvKMnM5Hb/sO7ct7XQ/ga/0sI/7gfsbUY8kqff8y2JJTTcYvylxb9bXf0+DQFJTDRs2jC1bthgGDZKZbNmyhWHDhvV6jl9eL6mpxo8fT1tbG5s3b252KfuMYcOGMX78+F6PNwgkNdXQoUOZOHFis8somqeGJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFa4hQRARcyPi6YjYEBGLa/RHRFxT9a+JiJlV+5ERsSIi1kXE2oi4pBH1SJJ6r+4giIghwLXAPGAy8PmImNxl2DxgUnVbBFxXtb8DfD0zjwVmA1+pMVeS1I8acUQwC9iQmc9m5lvAzcBZXcacBdyUHR4CRkZES2ZuysxHATJzK7AOGNeAmiRJvdSIIBgHbOy03caf/zLvcUxETABmAA83oCZJUi81IgiiRlv2ZUxEjABuAy7NzNdrPkjEoohYGRErN2/evMfFSpJ21YggaAOO7LQ9Hmjv7ZiIGEpHCCzLzJ919yCZeX1mtmZm69ixYxtQtiQJGhMEjwCTImJiROwPLATu7DLmTuAvq08PzQZey8xNERHA/wTWZebVDahFktRH+9W7g8x8JyIuBu4GhgA3ZObaiLiw6l8KLAfmAxuAbcAF1fTjgC8Aj0fE6qrt8sxcXm9dkqTeicyup/P3fq2trbly5cpmlyFJg0pErMrM1q7t/mWxJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFa0gQRMTciHg6IjZExOIa/RER11T9ayJiZm/nSpL6V91BEBFDgGuBecBk4PMRMbnLsHnApOq2CLiuD3MlSf2oEUcEs4ANmflsZr4F3Ayc1WXMWcBN2eEhYGREtPRyriSpHzUiCMYBGzttt1VtvRnTm7kARMSiiFgZESs3b95cd9GSpA6NCIKo0Za9HNObuR2NmddnZmtmto4dO7aPJUqSurNfA/bRBhzZaXs80N7LMfv3Yq4kqR814ojgEWBSREyMiP2BhcCdXcbcCfxl9emh2cBrmbmpl3MlSf2o7iOCzHwnIi4G7gaGADdk5tqIuLDqXwosB+YDG4BtwAW7m1tvTZKk3ovMmqfk92qtra25cuXKZpchSYNKRKzKzNau7f5lsSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSpcXUEQEYdGxD0Rsb76OaqbcXMj4umI2BARizu1fz8inoqINRFxe0SMrKceSVLf1XtEsBi4LzMnAfdV27uIiCHAtcA8YDLw+YiYXHXfA0zNzGnAH4C/qbMeSVIf1RsEZwE3VvdvBM6uMWYWsCEzn83Mt4Cbq3lk5q8y851q3EPA+DrrkST1Ub1BcHhmbgKofh5WY8w4YGOn7baqrau/Au6qsx5JUh/t19OAiLgXeF+Nrit6+RhRoy27PMYVwDvAst3UsQhYBHDUUUf18qElST3pMQgy8+Tu+iLipYhoycxNEdECvFxjWBtwZKft8UB7p32cB5wOzMnMpBuZeT1wPUBra2u34yRJfVPvqaE7gfOq++cBd9QY8wgwKSImRsT+wMJqHhExF7gMODMzt9VZiyRpD9QbBFcBp0TEeuCUapuIOCIilgNUbwZfDNwNrANuycy11fz/ARwM3BMRqyNiaZ31SJL6qMdTQ7uTmVuAOTXa24H5nbaXA8trjPtgPY8vSaqff1ksSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLh6gqCiDg0Iu6JiPXVz1HdjJsbEU9HxIaIWFyj/xsRkRExpp56JEl9V+8RwWLgvsycBNxXbe8iIoYA1wLzgMnA5yNicqf+I4FTgP9dZy2SpD1QbxCcBdxY3b8ROLvGmFnAhsx8NjPfAm6u5u3wD8A3gayzFknSHqg3CA7PzE0A1c/DaowZB2zstN1WtRERZwIvZOZjPT1QRCyKiJURsXLz5s11li1J2mG/ngZExL3A+2p0XdHLx4gabRkRw6t9nNqbnWTm9cD1AK2trR49SFKD9BgEmXlyd30R8VJEtGTmpohoAV6uMawNOLLT9nigHfgAMBF4LCJ2tD8aEbMy88U+rEGSVId6Tw3dCZxX3T8PuKPGmEeASRExMSL2BxYCd2bm45l5WGZOyMwJdATGTENAkgZWvUFwFXBKRKyn45M/VwFExBERsRwgM98BLgbuBtYBt2Tm2jofV5LUID2eGtqdzNwCzKnR3g7M77S9HFjew74m1FOLJGnP+JfFklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwkVmNruGPouIzcDzza5jD4wBXml2EQOotPWCay7FYF3z+zNzbNfGQRkEg1VErMzM1mbXMVBKWy+45lLsa2v21JAkFc4gkKTCGQQD6/pmFzDASlsvuOZS7FNr9j0CSSqcRwSSVDiDQJIKZxA0UEQcGhH3RMT66ueobsbNjYinI2JDRCyu0f+NiMiIGNP/Vden3jVHxPcj4qmIWBMRt0fEyAErvo968bxFRFxT9a+JiJm9nbu32tM1R8SREbEiItZFxNqIuGTgq98z9TzPVf+QiPi3iPjFwFVdp8z01qAb8PfA4ur+YuB7NcYMAZ4Bjgb2Bx4DJnfqPxK4m44/mBvT7DX195qBU4H9qvvfqzV/b7j19LxVY+YDdwEBzAYe7u3cvfFW55pbgJnV/YOBP+zra+7U/9fAPwO/aPZ6envziKCxzgJurO7fCJxdY8wsYENmPpuZbwE3V/N2+Afgm8BgeRe/rjVn5q8y851q3EPA+P4td4/19LxRbd+UHR4CRkZESy/n7o32eM2ZuSkzHwXIzK3AOmDcQBa/h+p5nomI8cBpwD8NZNH1Mgga6/DM3ARQ/TysxphxwMZO221VGxFxJvBCZj7W34U2UF1r7uKv6HiltTfqzRq6G9Pb9e9t6lnzThExAZgBPNz4Ehuu3jUvoeOF3L/3U339Yr9mFzDYRMS9wPtqdF3R213UaMuIGF7t49Q9ra2/9NeauzzGFcA7wLK+VTdgelzDbsb0Zu7eqJ41d3RGjABuAy7NzNcbWFt/2eM1R8TpwMuZuSoiTmx0Yf3JIOijzDy5u76IeGnHYXF1qPhyjWFtdLwPsMN4oB34ADAReCwidrQ/GhGzMvPFhi1gD/Tjmnfs4zzgdGBOVidZ90K7XUMPY/bvxdy9UT1rJiKG0hECyzLzZ/1YZyPVs+YFwJkRMR8YBhwSET/JzHP7sd7GaPabFPvSDfg+u75x+vc1xuwHPEvHL/0db0ZNqTHuOQbHm8V1rRmYCzwJjG32WnpYZ4/PGx3nhju/ifj7vjzne9utzjUHcBOwpNnrGKg1dxlzIoPozeKmF7Av3YDRwH3A+urnoVX7EcDyTuPm0/EpimeAK7rZ12AJgrrWDGyg43zr6uq2tNlr2s1a/2wNwIXAhdX9AK6t+h8HWvvynO+Ntz1dM3A8HadU1nR6buc3ez39/Tx32segCgIvMSFJhfNTQ5JUOINAkgpnEEhS4QwCSSqcQSBJhTMIpAEWEScOqitTap9nEEhS4QwCqRsRcW5E/D4iVkfEP1bXmX8jIv57RDwaEfdFxNhq7PSIeKjT9yqMqto/GBH3RsRj1ZwPVLsfERG3Vt/FsCyq64pIzWAQSDVExLHA54DjMnM68C5wDnAQ8GhmzgR+DVxZTbkJuCwzp9Hx16Y72pcB12bmR4D/BGyq2mcAlwKT6bj2/XH9vCSpW150TqptDvBR4JHqxfqBdFxQ79+Bf6nG/AT4WUS8FxiZmb+u2m8E/jUiDgbGZebtAJm5HaDa3+8zs63aXg1MAB7s91VJNRgEUm0B3JiZf7NLY8Tfdhm3u2u07O50z//rdP9d/L+oJvLUkFTbfcCCiDgMdn438/vp+D+zoBrzF8CDmfka8H8j4hNV+xeAX2fH9ffbIuLsah8HVN87Ie1VfBUi1ZCZT0bEt4BfRcR7gLeBrwB/AqZExCrgNTreRwA4D1ha/aJ/Frigav8C8I8R8XfVPv7LAC5D6hWvPir1QUS8kZkjml2H1EieGpKkwnlEIEmF84hAkgpnEEhS4QwCSSqcQSBJhTMIJKlw/x8ZvOMTei7ICQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 L2 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(13, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.dropout(F.relu(self.fc(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 Batch 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 교란 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 교란라벨 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisturbLabel(torch.nn.Module) :\n",
    "    def __init__(self, alpha, num_classes) :\n",
    "        super(DisturbLabel, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.C = num_classes\n",
    "        # 실제 라벨을 뽑을 확률\n",
    "        self.p_c = (1 - ((self.C-1)/self.C)*(alpha/100))\n",
    "        # 나머지\n",
    "        self.p_i = (1-self.p_c)/(self.C-1)\n",
    "        \n",
    "    def forward(self, y) :\n",
    "        # 앞서 언급한 라벨이 뽑힐 확률 분포 (3/100, 3/100, 3/100, 3/100, 3/100, 73/100, 3/100, 3/100, 3/100, 3/100)을 만들어준다. \n",
    "        y_tensor = y.type(torch.LongTensor).view(-1, 1)\n",
    "        depth = self.C\n",
    "        y_one_hot = torch.ones(y_tensor.size()[0], depth) * self.p_i\n",
    "        y_one_hot.scatter_(1, y_tensor, self.p_c)\n",
    "        \n",
    "        # 해당 확률을 이용해 Multinoulli 분포를 통해 샘플을 뽑는다.\n",
    "        y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,)))\n",
    "        distribution = torch.distributions.OneHoetCategorical(y_one_hot)\n",
    "        y_disturbed = distribution.sample()\n",
    "        \n",
    "        # 10개의 원소 중 가장 큰 값의 라벨을 뽑는다.\n",
    "        y_disturbed = y_disturbed.max(dim=1)[1]\n",
    "        return y_disturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 교란라벨 선언 및 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "disturblabels = DisturbLabel(alpha=30, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50) :\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader :\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        labels = disturblabels(labels).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        ...이하 생략..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 교란값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 노이즈 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 값과 noise 비율을 받는다.\n",
    "def noise_generator(x, alpha) :\n",
    "    # 임의로 정한 정규분포에 따른 노이즈 생성\n",
    "    noise = torch.normal(0, 1e-8, size=(len(x), 1))\n",
    "    # 노이즈 타깃이 아닌 값은 노이즈를 0으로 한다. \n",
    "    noise[torch.randint(0, len(x), (int(len(x)*(1-alpha)),))] = 0\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 교란값 선언 및 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(400) :\n",
    "    for data in trainloader :\n",
    "        inputs, values = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        values = values + noise_generator(values, alpha)\n",
    "        loss = criterion(outputs, values)\n",
    "        ...이하 생략..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 라벨 스무딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 라벨 스무딩 정의 및 선언하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module) : \n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1) :\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, pred, target) :\n",
    "        # cross-entropy 부분의 log softmax를 미리 계산한다. \n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad() : \n",
    "            # 예측값과 동일한 크기의 영텐서 만들기\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls-1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        # 위에서 만든 pred를 함께 사용해 cross entropy loss 함수를 계산\n",
    "        return torch.mean(torch.sum(-true_dis * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.CrossEntropyLoss() 대신, LabelSmoothingLoss로 criterion 선언\n",
    "criterion = LabelSmoothingLoss(classes=10, smoothing=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
