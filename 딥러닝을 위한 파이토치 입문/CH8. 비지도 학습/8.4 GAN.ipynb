{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vanilla GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `make_grid` : 격자 형태의 이미지를 만들게 한다. \n",
    "- `imageio` : gif 파일을 만들기 위해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 패션 아이템 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 티셔츠, 바지, 풀오버, 드레스, 코드, 샌들, 셔츠, 스니커즈, 가방, 앵클부츠로 구성된 FashionMNIST 데이터를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c4265f79c34ae9bb89eb3af2a05cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1730a07359b4ef8a9987877e2c7ae3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8fd48bd7774dbba8a1b46d0b33f2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d797f7e1a44163ab7b6e48ad839704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, ), (0.5, ))])\n",
    "trainset = FashionMNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 생성자 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `self.n_features`, `self.n_out`\n",
    "    - 생성자는 잠재변수로부터 784(28x28) 크기인 벡터를 생성한다.\n",
    "    - 따라서 잠재 변수의 크기를 임의로 정하고 출력 크기는 이미지를 일렬로 편 크기인 784로 정의\n",
    "- `self.linear`\n",
    "    - 기본 GAN은 nn.Linear를 사용하여 모델 구축\n",
    "    - 활성 함수로는 ReakyReLU (음수 구간의 양의 기울기를 주어 값을 계산 y=0.2x)\n",
    "- `x`\n",
    "    - 정의된 MLP를 거치고 784 크기의 벡터를 크기가 28x28인 흑백 이미지로 변경하여 새로운 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_features = 128\n",
    "        self.n_out = 784\n",
    "        self.linear = nn.Sequential(nn.Linear(self.n_features, 256),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    nn.Linear(256, 512),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    nn.Linear(512, 1024),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    # self.n_out 중요중요\n",
    "                                    nn.Linear(1024, self.n_out),\n",
    "                                    nn.Tanh())\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 구별자 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(Discriminator, self).__init__()\n",
    "        # 이미지를 일렬로 편 크기 784\n",
    "        self.n_in = 784\n",
    "        \n",
    "        # 진위여부를 판단하기 위해 하나의 숫자로 정의\n",
    "        self.n_out = 1\n",
    "        # 구조를 정의\n",
    "        self.linear = nn.Sequential(nn.Linear(self.n_in, 1024),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    nn.Dropout(0.3),\n",
    "                                    nn.Linear(1024, 512),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    nn.Dropout(0.3),\n",
    "                                    nn.Linear(512, 256),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    nn.Dropout(0.3),\n",
    "                                    nn.Linear(256, self.n_out),\n",
    "                                    # 0과 1 사이의 값이 출력될 수 있도록\n",
    "                                    nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        # 이미지를 벡터 형태로 변경하여 신경망에 넣는다. \n",
    "        x = x.view(-1, 784)\n",
    "        x = self.linear(x)\n",
    "        return x                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자와 구별자 각각 선언\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 손실함수 및 최적화 기법 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optim = optim.Adam(generator.parameters(), lr=2e-4)\n",
    "d_optim = optim.Adam(discriminator.parameters(), lr=2e-4)\n",
    "\n",
    "# 학습 동안 손실 함수값과 샘플 이미지 저장을 위해 빈 리스트를 만든다.\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "images = []\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 잠재변수 및 라벨 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `noise`\n",
    "    - 기본적으로 크기가 128인 잠재 변수 n개를 무작위로 생성한다.\n",
    "    - 손실함수에서 이미지의 진위 여부에 대한 계산을 하게 되므로, 실제 이미지의 클래스를 사용하지 않고 진짜 데이터는 라벨을 1로 생성자로부터 만들어진 이미지의 라벨을 0으로 정의하여 사용\n",
    "- `label_ones`, `label_zeros`\n",
    "    - 위를 위해 1과 0 라벨을 만들어주는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n, n_features=128) :\n",
    "    data = torch.randn(n, n_features)\n",
    "    return data.to(device)\n",
    "\n",
    "def label_ones(size) :\n",
    "    data = torch.ones(size, 1) \n",
    "    return data.to(device)\n",
    "\n",
    "def label_zeros(size) :\n",
    "    data = torch.zeros(size, 1)\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 구별자 학습 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진짜 이미지, 가짜 이미지, 최적화 함수를 받는다. \n",
    "def train_discriminator(optimizer, real_data, fake_data) :\n",
    "    # 각 이미지의 진위 라벨을 할당하기 위해서 이미지의 개수 확인\n",
    "    n = real_data.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 진짜 이미지 판별\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # 이미지 수만큼 1라벨을 넣어 손실 함수를 계산\n",
    "    d_loss = criterion(prediction_real, label_ones(n))\n",
    "    \n",
    "    # 가짜 이미지 판별\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # 이미지 수만큼 0라벨을 넣어 손실 함수를 계산\n",
    "    g_loss = criterion(prediction_fake, label_zeros(n))\n",
    "    \n",
    "    loss = d_loss + g_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 생성자 학습 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가짜 이미지, 최적화 함수를 받는다.\n",
    "def train_generator(optimizer, fake_data) :\n",
    "    # 이미지의 개수 저장\n",
    "    n = fake_data.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    # 가짜 이미지를 넣어 판별\n",
    "    prediction = discriminator(fake_data)\n",
    "    # 생성자 입장에서는 구별자가 가짜 이미지를 진짜 이미지라고 판단하도록 업데이트가 되어야 하므로\n",
    "    # 0 라벨이 아니라 1라벨을 넣어 손실 함수 계산\n",
    "    loss = criterion(prediction, label_ones(n))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(noise(100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:g_loss:0.728 d_loss:1.401\n",
      "Epoch 10:g_loss:0.728 d_loss:1.401\n",
      "Epoch 20:g_loss:0.728 d_loss:1.401\n",
      "Epoch 30:g_loss:0.728 d_loss:1.401\n",
      "Epoch 40:g_loss:0.728 d_loss:1.401\n",
      "Epoch 50:g_loss:0.728 d_loss:1.401\n",
      "Epoch 60:g_loss:0.728 d_loss:1.401\n",
      "Epoch 70:g_loss:0.728 d_loss:1.401\n",
      "Epoch 80:g_loss:0.728 d_loss:1.401\n",
      "Epoch 90:g_loss:0.728 d_loss:1.401\n",
      "Epoch 100:g_loss:0.728 d_loss:1.401\n",
      "Epoch 110:g_loss:0.728 d_loss:1.401\n",
      "Epoch 120:g_loss:0.728 d_loss:1.401\n",
      "Epoch 130:g_loss:0.728 d_loss:1.401\n",
      "Epoch 140:g_loss:0.728 d_loss:1.401\n",
      "Epoch 150:g_loss:0.728 d_loss:1.401\n"
     ]
    }
   ],
   "source": [
    "# 검증을 위한 무작위 잠재 변수 64개 생성 : torch.Size([64, 128])\n",
    "test_noise = noise(64)\n",
    "\n",
    "# 평균 손실값을 구하는데 사용하는 배치 수 저장 (600)\n",
    "l = len(trainloader)\n",
    "\n",
    "for epoch in range(151) :\n",
    "    # 각 에폭마다 손실값을 초기화\n",
    "    g_loss = 0.0\n",
    "    d_loss = 0.0\n",
    "    for data in trainloader :\n",
    "        # 진짜 이미지를 받고 진짜 이미지의 개수를 정의 (100개)\n",
    "        imgs, _ = data\n",
    "        n = len(imgs)\n",
    "        \n",
    "        # 진짜 이미지의 개수만큼 가짜 이미지를 생성, detach()를 통해 불필요한 requires_grad 비활성화\n",
    "        fake_data = generator(noise(n)).detach() \n",
    "        real_data = imgs.to(device)\n",
    "        # 가지고 있는 이미지를 모두 구별자에 넣어 판별하고, 구별자를 업데이트하고 반환한 손실함수 값을 누적\n",
    "        d_loss += train_discriminator(d_optim, real_data, fake_data)\n",
    "        \n",
    "        \n",
    "        # 생성자를 업데이트하고 반환한 손실 함수값을 누적\n",
    "        fake_data = generator(noise(n))\n",
    "        g_loss += train_generator(g_optim, fake_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # 검증을 위해 고정된 잠재 변수를 넣어 \"이미지를 생성\"\n",
    "    # 변화를 알아보기 위해 격자 형태의 이미지로 만들어 \"images 리스트에 저장\"\n",
    "    img = generator(test_noise).detach().cpu()\n",
    "    img = make_grid(img)\n",
    "    images.append(img)\n",
    "    # 손실 함수값의 변화 저장\n",
    "    g_losses.append(g_loss/l)\n",
    "    d_losses.append(d_loss/l)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {}:g_loss:{:.3f} d_loss:{:.3f}\\r\" .format(epoch, g_loss/l, d_loss/l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discriminator.state_dict(), './model/fmnist_disc.pth')\n",
    "torch.save(generator.state_dict(), './model/fmnist_gner.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 검증 이미지 변화를 gif 파일로 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- images 리스트의 원소 하나는 격자 형태로 만들어진 이미지들의 모임\n",
    "    - 크기 : (3, 242, 242)인 하나의 텐서\n",
    "    - 이미지 저장 형식을 맞추기 위해 ToPILImage()를 이용해 타입, 크기(242, 242, 3)으로 크기를 변환할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_image = transforms.ToPILImage()\n",
    "\n",
    "# 이미지를 넘파이 배열로 변경하여 gif 파일로 만든다. \n",
    "imgs = [np.array(to_image(i)) for i in images]\n",
    "imageio.mimsave('fashion_items.gif', imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 손실함수값 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(g_losses)\n",
    "plt.plot(d_losses)\n",
    "plt.legend(['Generator', 'Discriminator'])\n",
    "plt.title(\"Loss\")\n",
    "plt.savefig('gan_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deep Convolutional GAN (DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DCGAN : GAN 구조를 합성곱 층으로 구성한 모델\n",
    "- 합성곱 신경망은 MLP보다 이미지 처리에 매우 유리한 네트워크\n",
    "- 실제 MLP로 구성된 GAN보다 DCGAN을 통해 더 선명한 이미지를 생성할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 생성자 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x : 크기가 128인 잠재 변수를 가지고 채널이 128인 1x1 크기 이미지를 입력값으로 받는다. \n",
    "- 28x28 이미지 한 장을 얻기 위해 nn.ConvTranspose2d \n",
    "    - 입력 성분(Conv2의 결과)을 출력 성분(Conv2의 입력)으로 미분하여 그 값을 입력 벡터와 곱해 출력 벡터를 산출하고, 그 결과 벡터를 행렬 형태로 변환하는 연산\n",
    "    - **(입력 채널 수, 출력 채널 수, 필터 크기, stride, padding)**을 입력\n",
    "    - 크기는 일반적인 정사각형 이미지와 필터를 사용했을 경우, 다음 식에 의해 피쳐맵의 크기를 산출\n",
    "    - **(출력값의 크기) = (입력값의 크기 - 1)x(보폭) - 2x(패딩) + (필터의 크기) + (출력값 패딩)**\n",
    "    - 예를 들어 입력값이 1x1 이미지이고, (stride=1, padding=0, filter size = 3x3, padding=0)이면 출력값의 크기는 3이 되고, 계산과정을 반복하면 28x28 이미지 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_features = 128\n",
    "        self.conv = nn.Sequential(nn.ConvTranspose2d(self.n_features, 256, 3, 1, bias=True),\n",
    "                                  nn.ReLU(True),\n",
    "                                  nn.ConvTranspose2d(256, 128, 3, 2, bias=False),\n",
    "                                  nn.ReLU(True),\n",
    "                                  nn.ConvTranspose2d(128, 64, 3, 2, bias=False),\n",
    "                                  nn.ReLU(True),\n",
    "                                  nn.ConvTranspose2d(64, 1, 2, 2, 1, bias=False),\n",
    "                                  nn.Tanh())\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        # 입력값이 1x1 이미지 (생소할 수도 있다.)\n",
    "        x = x.view(-1, self.n_features, 1, 1)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 256, 3, 3]         295,168\n",
      "              ReLU-2            [-1, 256, 3, 3]               0\n",
      "   ConvTranspose2d-3            [-1, 128, 7, 7]         294,912\n",
      "              ReLU-4            [-1, 128, 7, 7]               0\n",
      "   ConvTranspose2d-5           [-1, 64, 15, 15]          73,728\n",
      "              ReLU-6           [-1, 64, 15, 15]               0\n",
      "   ConvTranspose2d-7            [-1, 1, 28, 28]             256\n",
      "              Tanh-8            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 664,064\n",
      "Trainable params: 664,064\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 2.53\n",
      "Estimated Total Size (MB): 2.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(generator, input_size=(100, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 구별자 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(1, 128, 3, 2, 1, bias=False),\n",
    "                                  nn.LeakyReLU(0.2),\n",
    "                                  nn.Dropout(0.5),\n",
    "                                  nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
    "                                  nn.LeakyReLU(0.2),\n",
    "                                  nn.Dropout(0.5),\n",
    "                                  nn.Conv2d(256, 256, 3, 2, 1, bias=False),\n",
    "                                  nn.LeakyReLU(0.2, inplace=True),\n",
    "                                  nn.Dropout(0.5),\n",
    "                                  nn.Conv2d(256, 1, 3, 2, bias=False),\n",
    "                                  nn.sigmoid())\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.conv(x)\n",
    "        return x.view(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
