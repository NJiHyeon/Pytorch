{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n1. View example figures with label in 3 x 3 grids\n2. Use data augmentation to increase data size\n3. Use callback to monitor training.\n4. Apply early stopping, learning rate scheduling.\n5. Batch normalization\n6. Plot the accuracy vs epoch, and loss vs epoch for training and testing data.","metadata":{"id":"Maluf4BOJwQs"}},{"cell_type":"code","source":"# Import TensorFlow and TensorFlow Datasets\nimport tensorflow_datasets as tfds\nimport warnings\nwarnings.filterwarnings('ignore') \nimport tensorflow as tf\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"","metadata":{"id":"0Y9DZbQHJwgd","execution":{"iopub.status.busy":"2022-09-29T06:59:35.961393Z","iopub.execute_input":"2022-09-29T06:59:35.962055Z","iopub.status.idle":"2022-09-29T06:59:45.438759Z","shell.execute_reply.started":"2022-09-29T06:59:35.961933Z","shell.execute_reply":"2022-09-29T06:59:45.437413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale(image, label):\n    image = tf.cast(image, tf.float32)\n    image /= 255\n    return image, label\n\n\ndef get_data():\n    #Download the dataset and divide it into train and test\n    datasets, info = tfds.load('beans', with_info=True, as_supervised=True)\n    beans_train,beans_val= datasets['train'],datasets['validation']\n\n    BUFFER_SIZE = 10000\n\n    BATCH_SIZE_PER_REPLICA = 32\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA# * strategy.num_replicas_in_sync\n    train_dataset = beans_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n    val_dataset = beans_val.map(scale).batch(BATCH_SIZE)\n#     test_dataset = beans_test.map(scale).batch(BATCH_SIZE)\n    return beans_train,beans_val, train_dataset,val_dataset,info\n\nbeans_train,beans_val, train_dataset,val_dataset,info = get_data()","metadata":{"execution":{"iopub.status.busy":"2022-09-29T07:00:12.772581Z","iopub.execute_input":"2022-09-29T07:00:12.774434Z","iopub.status.idle":"2022-09-29T07:00:20.292675Z","shell.execute_reply.started":"2022-09-29T07:00:12.774385Z","shell.execute_reply":"2022-09-29T07:00:20.291796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# 1. View example figures with label in 3 x 3 grids\nimages = iter(beans_train)\nnum_classes = info.features['label'].num_classes\nprint(f\"total {num_classes} classes.\")\nget_label_name = info.features['label'].int2str\ni = 0\nfig = plt.figure(figsize=(10,9))\nfor image,label in  iter(beans_train):\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.imshow(image)\n    plt.title(get_label_name(label))\n    ax.axis(\"off\")\n    i+=1\n    if i>=9:\n        break\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-29T07:01:08.054484Z","iopub.execute_input":"2022-09-29T07:01:08.054994Z","iopub.status.idle":"2022-09-29T07:01:09.297739Z","shell.execute_reply.started":"2022-09-29T07:01:08.054953Z","shell.execute_reply":"2022-09-29T07:01:09.296387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(learning_rate, num_dense_layers, activation,num_classes= 3):\n    # Make CNN\n    model = tf.keras.Sequential([\n                            #     2. Use data augmentation to increase data size\n                            tf.keras.Sequential([\n                                  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n                                  tf.keras.layers.RandomRotation(0.2),\n                                ]),\n                            # COV2D layer\n                            tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(500, 500, 3)),\n                            #     6. Batch normalization\n                            tf.keras.layers.BatchNormalization(),\n                            tf.keras.layers.MaxPooling2D(),\n                            tf.keras.layers.Conv2D(32, 3, activation='relu'),\n    #                         tf.keras.layers.BatchNormalization(),\n                            tf.keras.layers.MaxPooling2D(),\n                            tf.keras.layers.Conv2D(64, 3, activation='relu'),\n    #                         tf.keras.layers.BatchNormalization(),\n                            tf.keras.layers.MaxPooling2D(),\n                            tf.keras.layers.Flatten(),\n                            ])\n    for i in range(num_dense_layers):\n        model.add(tf.keras.layers.Dense(32, activation=activation))\n    model.add(tf.keras.layers.Dense(num_classes,activation='softmax'))\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n            metrics=['accuracy'])\n    return model","metadata":{"id":"PGzE4tFEUYYS","execution":{"iopub.status.busy":"2022-09-29T07:01:20.741513Z","iopub.execute_input":"2022-09-29T07:01:20.741923Z","iopub.status.idle":"2022-09-29T07:01:20.754413Z","shell.execute_reply.started":"2022-09-29T07:01:20.741890Z","shell.execute_reply":"2022-09-29T07:01:20.752895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nglobal learning_rate\n\nlearning_rate, num_dense_layers, activation = 1e-4,1,'relu'\nprint(learning_rate, num_dense_layers, activation)\n# Define the checkpoint directory to store the checkpoints\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n  \n# Callback for printing the LR at the end of each epoch.\nclass PrintLR(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n                                                      model.optimizer.lr.numpy()))\n\n# 5. Apply early stopping, learning rate scheduling. \n# Function for decaying the learning rate.\ndef decay(epoch):\n    if epoch < 3:\n        return learning_rate * 10\n    elif epoch >= 3 and epoch < 7:\n        return learning_rate\n    else:\n        return learning_rate / 10\n\n# 4. Use callback to monitor your training.\n# Define the callbacks\ncallbacks = [\n#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n#     tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n#                                        save_weights_only=True),\n    tf.keras.callbacks.LearningRateScheduler(decay),\n    # 5. Apply early stopping, learning rate scheduling. \n    tf.keras.callbacks.EarlyStopping(patience=10, monitor='loss', mode='min', verbose=1, restore_best_weights=False),\n    PrintLR()\n]\n\n# train the model using the best hyper params\nstart = time.time()\n# create model\nmodel = create_model(learning_rate, num_dense_layers, activation)\n# train model\nhistory = model.fit(train_dataset, epochs = 50, \n          validation_data = val_dataset,\n          callbacks=callbacks)\nend = time.time()\nprint(\"Time elapsed: {}\".format(end-start))","metadata":{"id":"3DLOvhyDUcOk","outputId":"2f9eea8d-5e4a-4624-98c7-ec2d7656e899","execution":{"iopub.status.busy":"2022-09-29T07:02:33.251134Z","iopub.execute_input":"2022-09-29T07:02:33.251527Z","iopub.status.idle":"2022-09-29T07:13:38.300405Z","shell.execute_reply.started":"2022-09-29T07:02:33.251494Z","shell.execute_reply":"2022-09-29T07:13:38.297758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7. Plot the accuracy vs epoch, and loss vs epoch for training and testing data.\nimport matplotlib.pyplot as plt\n# get training history\nloss = history.history['loss']\nacc = history.history['accuracy']\nval_loss = history.history['val_loss']\nval_acc = history.history['val_accuracy']\nx_epoch = list(range(1,len(loss)+1))\n# plot the figure\nfig = plt.figure(figsize=(10,6))\nax1 = fig.add_subplot(1,1,1)\nax2 = ax1.twinx()\nax1.plot(x_epoch,acc,'r-*',label = 'train_accuracy')\nax2.plot(x_epoch,loss,'g-*',label = 'train_loss')\nax1.plot(x_epoch,val_acc,'r-o',label = 'val_accuracy')\nax2.plot(x_epoch,val_loss,'g-o',label = 'val_loss')\nax1.legend(loc = 2)\nax2.legend(loc = 0)\nax1.set_ylabel('Accuracy')\nax2.set_ylabel('Loss')\nax1.set_xlabel('Epoches')\nplt.title(\"History\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"1K-fiNbnUpNu","execution":{"iopub.status.busy":"2022-09-29T07:13:38.301870Z","iopub.status.idle":"2022-09-29T07:13:38.302313Z","shell.execute_reply.started":"2022-09-29T07:13:38.302107Z","shell.execute_reply":"2022-09-29T07:13:38.302127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}