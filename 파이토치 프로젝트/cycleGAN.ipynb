{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1451d47-89e8-4eba-ae96-7d657d52a073",
   "metadata": {},
   "source": [
    "### 1. 전처리 및 데이터 클래스 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94b311-0505-49b1-8cbc-13b61e1bf1a3",
   "metadata": {},
   "source": [
    "#### 1) Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66a48e4-fef6-4884-90cd-e6cbf762aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb1d10-de7b-46e2-bec5-4e6d1f1920b5",
   "metadata": {},
   "source": [
    "#### 2) 흑백 이미지를 RGB 이미지로 바꾸는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57ae83-c35c-4771-9028-9a025dc409e3",
   "metadata": {},
   "source": [
    "`to_rgb` : PIL의 Image 모듈을 이용해 인자로 받은 흑백 이미지를 RGB로 바꿔서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3629f6-33f5-47aa-9cd6-160121d78d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(image) :\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229186d8-2741-4035-929a-8b9713584071",
   "metadata": {},
   "source": [
    "#### 3) 사용자 정의 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fbe5b1-117b-4062-b5e4-39e53fe12906",
   "metadata": {},
   "source": [
    "- 정의한 데이터셋으로 Loader를 이용해서 배치 사이즈만큼 이미지를 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a07692-b662-4e05-93da-abe11ece3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset) :\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\") :\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if mode == \"train\" :\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root, \"trainA\") + \"/*.*\"))\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root, \"trianB\") + \"/*.*\"))\n",
    "        else :\n",
    "            self.fiels_A = sorted(glob.glob(os.path.join(root, \"testA\") + \"/*.*\"))\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root, \"testB\") + \"/*.*\"))\n",
    "            \n",
    "    def __getitem__(self, index) :\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "        \n",
    "        \n",
    "        # unaligned 변수로 학습할 쌍을 무작위로 고를지 고정시킬지 ?\n",
    "        if self.unaligned :\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B)-1)])\n",
    "        else :\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "            \n",
    "        \n",
    "        if image_A.mode != \"RGB\" :\n",
    "            image_A = to_rgb(image_A)\n",
    "        if image_B.mode != \"RGB\" :\n",
    "            image_B = to_rgb(image_B)\n",
    "            \n",
    "        # 불러온 PIL 이미지를 인자로 받은 transform 함수를 적용하여 torch의 tensor 자료형으로 바꾼다. \n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {\"A\" : item_A, \"B\" : item_B}\n",
    "    \n",
    "    \n",
    "    def __len__(self) :\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f125f-2241-484f-83a4-084816b94fb5",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6213c-a554-4378-9542-99fb8cd5e302",
   "metadata": {},
   "source": [
    "### 2. Generator 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79381dd1-5c01-4d65-a717-3002cb8b386b",
   "metadata": {},
   "source": [
    "- Generator는 이미지를 입력받아 다른 스타일의 이미지를 생성하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91732dbb-a785-4808-bfda-9d75f25a6135",
   "metadata": {},
   "source": [
    "#### 1) 가중치 초기화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f137b82-7f8f-4b2c-8eb1-1d5260c166ad",
   "metadata": {},
   "source": [
    "- 토치에서 제공하는 layer의 종류에 따라 가중치 초기화를 다르게 하고자 아래의 함수를 사용\n",
    "- __class__로 layer의 이름을 얻은 후에 각 종류에 맞게 가중치를 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f94d82-c6a4-4346-9796-027649be90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m) :\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1 :\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None :\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "    elif classname.find(\"BatchNorm2d\") != -1 :\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd818191-cfd1-46d3-bc8e-f2ae15539748",
   "metadata": {},
   "source": [
    "#### 2) Residual block 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e37a53-b3c3-496b-bbf1-57706411d5f1",
   "metadata": {},
   "source": [
    "- Generator를 구현하기 위해서는 내부에 들어갈 layer인 Residual Block을 구현해야 한다.\n",
    "- Residual Block \n",
    "    - 이전 layer와 현재 layer의 출력값을 더해서 Forward\n",
    "    - 모델이 깊어짐에 따라 생기는 기울기 소실 문제 해결\n",
    "    - 더하기 연산으로는 기울기가 작아지지 않고 그대로 Back Propagation이 일어나기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd1454b-4ee7-484e-aa64-d237fac44b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module) :\n",
    "    def __init__(self, in_features) :\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "    def forward(self, x) :\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90240d04-3bd1-4d6f-98cb-a8d70e6cbef0",
   "metadata": {},
   "source": [
    "- `ReflectionPad` : 점대칭 방식으로 가장 가까운 픽셀로부터 값을 복사\n",
    "    - Zero Padding처럼 값 지정이 아닌 더욱 자연스러운 이미지 생성을 위해 사용\n",
    "- `Instance Normalization` : 데이터 개별로 정규화 진행\n",
    "    - 정규화 : 데이터 값을 범위를 비슷하게 조정\n",
    "    - 배치 정규화는 데이터의 배치 단위로 평균과 분산을 구하여 학습의 안정성을 높이지만, Instance Normalization은 이미지에 특화된 정규화 과정으로 이미지를 개별로 정규화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c12713-65f3-4903-bbbf-50161d3a7de0",
   "metadata": {},
   "source": [
    "#### 3) Generator 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bbfe7-cb50-43d5-8cda-b50aa2154699",
   "metadata": {},
   "source": [
    "- 입력 이미지를 down-sampling한 후, \n",
    "- 여러개의 Residual Block을 통과시킨 후에\n",
    "- Up-sampling하는 것으로 스타일을 변환하는 Generator 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fed1f75-296f-4f39-b7e9-6ee48d1e1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module) :\n",
    "    def __init__(self, input_shape, num_residual_blocks) :\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "        \n",
    "        # 초기 Convolutional block 선언\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        \n",
    "        # Downsampling을 2번 진행한다. (stride=2이므로 이미지의 크기가 반씩 줄어든다.)\n",
    "        for _ in range(2) :\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            \n",
    "        \n",
    "        # num_residual_blocks만큼 residual block 만들기\n",
    "        for _ in range(num_residual_blocks) :\n",
    "            model += [ResidualBlock(out_features)]\n",
    "            \n",
    "            \n",
    "        # nn.Upsample을 2번 진행하여 다시 이미지의 크기를 2배씩 늘린다.\n",
    "        for _ in range(2) :\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            \n",
    "            \n",
    "        # 출력 layer를 선언한다. (출력 이미지의 크기 = 입력 이미지의 크기)\n",
    "        # 마지막 층의 활성 함수는 nn.Tanh() 사용\n",
    "        model += [nn.ReflectionPad2d(channels), \n",
    "                  nn.Conv2d(out_features, channels, 7),\n",
    "                  nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    \n",
    "    def forward(self, x) :\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa665b7e-55af-47dd-a9ac-e35d17e3bf36",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b8733-bac3-4f24-b0c3-f81cc8818078",
   "metadata": {},
   "source": [
    "### 3. Discriminator 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f7f69-0b57-4055-9520-a9f528c8ee09",
   "metadata": {},
   "source": [
    "- 입력받은 이미지가 실제 이미지인지 생성된 이미지인지를 분류하는 역할\n",
    "- (PatchGAN의 Discriminator 기반) \n",
    "    - 정사각형 사이즈의 이미지 패치 영역에 대하여 생성된 이미지가 가짜인지 진짜인지 판단\n",
    "    - 이미지 영역을 분할하지 않는다면, Generator는 우리가 학습하려는 스타일의 변환이 아닌 엉뚱한 특징으로 Discriminator를 속인다.\n",
    "    - 이를 방지하기 위해 각각의 패치 영역을 따로 판단하여 원하는 스타일의 변환을 학습할 수 있다.\n",
    "    - 또한 작은 이미지 패치에 대하여 연산을 수행하므로 파라미터의 개수가 작아지고 속도가 더 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd48f7-34a8-4492-89cf-677804e16c1d",
   "metadata": {},
   "source": [
    "#### 1) Discriminator 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91fa4fab-d02c-40f6-8380-a43422369913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module) :\n",
    "    def __init__(self, input_shape) :\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        \n",
    "        \n",
    "        # Discriminator의 출력 크기를 정한다.\n",
    "        # PatchGAN의 Discriminator는 출력이 0또는 1의 값이 아니라 입력 이미지의 1/16인 이진화된 Feature Map\n",
    "        # 만약 이미지의 크기가 256x256이라면 Discriminator의 출력 이미지는 16x16\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "        \n",
    "        \n",
    "        # discriminator block은 stride=2로 점점 Downsampling하면서 출력 이미지의 크기를 줄인다. \n",
    "        def discriminator_block(in_filters, out_filters, normalize=True) :\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize :\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        \n",
    "        # 이미지의 크기가 256x256일 때, discriminator_block을 4번 통과하면 한번 통과할 때마다 크기가 반으로 줄어들므로,\n",
    "        # 출력 이미지의 크기는 16x16\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "                \n",
    "        \n",
    "        \n",
    "    def forward(self, img) :\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734e0f1-3799-4d40-afd7-29b2c74999b8",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927ac0f-5c8f-4929-9688-6be2d5458257",
   "metadata": {},
   "source": [
    "### 4. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81757bdf-a4b4-423d-b8e2-0593be9afda6",
   "metadata": {},
   "source": [
    "#### 1) 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c097b-1ca2-47bb-98ea-9bfcfcaccd8f",
   "metadata": {},
   "source": [
    "- `dataset_name` : 학습 및 테스트 데이터가 들어 있는 폴더 혹은 폴더의 이름\n",
    "- `channels` : 이미지의 채널(흑백 : 1, RGB 이미지 : 3)\n",
    "- `img_height`, `img_width` : 이미지의 가로, 세로의 크기\n",
    "- `n_residual_blocks` : Generator에서의 Residual Block의 개수\n",
    "- `lr` : 모델에 대한 Learning Rate\n",
    "- `b1`, `b2`  : Adam Optimizer에 대한 HyperParameter\n",
    "- `lambda_cyc`, `lambda_id` : Cycle-consistency Loss와 Identity Loss에 대한 람다 값\n",
    "    - lambda_id가 클수록 본래의 색감을 유지하려는 성질\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b01e5fe4-7c47-4950-87b3-5409422d4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"selfie2anime\"\n",
    "channels = 3\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "n_residual_blocks = 9\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "n_epochs = 200\n",
    "init_epoch = 0\n",
    "decay_epoch = 100\n",
    "lambda_cyc = 10.0\n",
    "lambda_id = 5.0\n",
    "n_cpu = 8\n",
    "batch_size = 1\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6ca0d-c792-4888-b3fe-3bf20b74a7ce",
   "metadata": {},
   "source": [
    "#### 2) 샘플 이미지와 모델 가중치를 저장할 폴더 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2194f-cb2d-447e-83d0-51b65e960fda",
   "metadata": {},
   "source": [
    "- 학습하는 동안 생성할 샘플 이미지와 학습시킬 모델을 저장할 폴더 생성\n",
    "- `exist_ok = True`로 지정하면 같은 이름의 폴더가 있어도 오류가 나지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bc7b37-0c06-4cc4-9469-da6e69775c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7795be-a791-42f0-9a75-e68b9b11de64",
   "metadata": {},
   "source": [
    "#### 3) 손실 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab17dcfb-781b-4b0a-a87e-ea74c1700cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ba2f-4420-483c-9f73-3129ccefd595",
   "metadata": {},
   "source": [
    "#### 4) 모델 객체 선언하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16b224-046d-4393-8839-ac74ba9a4f28",
   "metadata": {},
   "source": [
    "- 스타일을 변환하기 위한 Generator, Discriminator 선언\n",
    "    - G_AB : A에서 B로 변환 \n",
    "    - G_BA : B에서 A로 변환\n",
    "    - D_A,D_B : 생성한 스타일 A, B가 진짜인지 가짜인지 판별할 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2081db0-f920-44a8-bcee-e3a6df422416",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d640584b-29da-4b10-919d-380be4f7202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b432a4f8-97ae-4d41-a270-6efcd0c85dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "GeneratorResNet                               [1, 3, 256, 256]          --\n",
       "├─Sequential: 1-1                             [1, 3, 256, 256]          --\n",
       "│    └─ReflectionPad2d: 2-1                   [1, 3, 262, 262]          --\n",
       "│    └─Conv2d: 2-2                            [1, 64, 256, 256]         9,472\n",
       "│    └─InstanceNorm2d: 2-3                    [1, 64, 256, 256]         --\n",
       "│    └─ReLU: 2-4                              [1, 64, 256, 256]         --\n",
       "│    └─Conv2d: 2-5                            [1, 128, 128, 128]        73,856\n",
       "│    └─InstanceNorm2d: 2-6                    [1, 128, 128, 128]        --\n",
       "│    └─ReLU: 2-7                              [1, 128, 128, 128]        --\n",
       "│    └─Conv2d: 2-8                            [1, 256, 64, 64]          295,168\n",
       "│    └─InstanceNorm2d: 2-9                    [1, 256, 64, 64]          --\n",
       "│    └─ReLU: 2-10                             [1, 256, 64, 64]          --\n",
       "│    └─ResidualBlock: 2-11                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-1                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-12                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-2                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-13                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-3                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-14                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-4                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-15                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-5                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-16                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-6                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-17                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-7                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-18                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-8                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-19                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-9                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─Upsample: 2-20                         [1, 256, 128, 128]        --\n",
       "│    └─Conv2d: 2-21                           [1, 128, 128, 128]        295,040\n",
       "│    └─InstanceNorm2d: 2-22                   [1, 128, 128, 128]        --\n",
       "│    └─ReLU: 2-23                             [1, 128, 128, 128]        --\n",
       "│    └─Upsample: 2-24                         [1, 128, 256, 256]        --\n",
       "│    └─Conv2d: 2-25                           [1, 64, 256, 256]         73,792\n",
       "│    └─InstanceNorm2d: 2-26                   [1, 64, 256, 256]         --\n",
       "│    └─ReLU: 2-27                             [1, 64, 256, 256]         --\n",
       "│    └─ReflectionPad2d: 2-28                  [1, 64, 262, 262]         --\n",
       "│    └─Conv2d: 2-29                           [1, 3, 256, 256]          9,411\n",
       "│    └─Tanh: 2-30                             [1, 3, 256, 256]          --\n",
       "===============================================================================================\n",
       "Total params: 11,378,179\n",
       "Trainable params: 11,378,179\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 56.83\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 261.62\n",
       "Params size (MB): 45.51\n",
       "Estimated Total Size (MB): 307.92\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 1\n",
    "summary(GeneratorResNet(input_shape=(3, 256, 256), num_residual_blocks=9), input_size = (batch_size, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1aacdfc-a615-4e28-aeee-4b75d1ed5a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "GeneratorResNet                               [1, 3, 256, 256]          --\n",
       "├─Sequential: 1-1                             [1, 3, 256, 256]          --\n",
       "│    └─ReflectionPad2d: 2-1                   [1, 3, 262, 262]          --\n",
       "│    └─Conv2d: 2-2                            [1, 64, 256, 256]         9,472\n",
       "│    └─InstanceNorm2d: 2-3                    [1, 64, 256, 256]         --\n",
       "│    └─ReLU: 2-4                              [1, 64, 256, 256]         --\n",
       "│    └─Conv2d: 2-5                            [1, 128, 128, 128]        73,856\n",
       "│    └─InstanceNorm2d: 2-6                    [1, 128, 128, 128]        --\n",
       "│    └─ReLU: 2-7                              [1, 128, 128, 128]        --\n",
       "│    └─Conv2d: 2-8                            [1, 256, 64, 64]          295,168\n",
       "│    └─InstanceNorm2d: 2-9                    [1, 256, 64, 64]          --\n",
       "│    └─ReLU: 2-10                             [1, 256, 64, 64]          --\n",
       "│    └─ResidualBlock: 2-11                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-1                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-12                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-2                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-13                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-3                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-14                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-4                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-15                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-5                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-16                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-6                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-17                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-7                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-18                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-8                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─ResidualBlock: 2-19                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-9                   [1, 256, 64, 64]          1,180,160\n",
       "│    └─Upsample: 2-20                         [1, 256, 128, 128]        --\n",
       "│    └─Conv2d: 2-21                           [1, 128, 128, 128]        295,040\n",
       "│    └─InstanceNorm2d: 2-22                   [1, 128, 128, 128]        --\n",
       "│    └─ReLU: 2-23                             [1, 128, 128, 128]        --\n",
       "│    └─Upsample: 2-24                         [1, 128, 256, 256]        --\n",
       "│    └─Conv2d: 2-25                           [1, 64, 256, 256]         73,792\n",
       "│    └─InstanceNorm2d: 2-26                   [1, 64, 256, 256]         --\n",
       "│    └─ReLU: 2-27                             [1, 64, 256, 256]         --\n",
       "│    └─ReflectionPad2d: 2-28                  [1, 64, 262, 262]         --\n",
       "│    └─Conv2d: 2-29                           [1, 3, 256, 256]          9,411\n",
       "│    └─Tanh: 2-30                             [1, 3, 256, 256]          --\n",
       "===============================================================================================\n",
       "Total params: 11,378,179\n",
       "Trainable params: 11,378,179\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 56.83\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 261.62\n",
       "Params size (MB): 45.51\n",
       "Estimated Total Size (MB): 307.92\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 1\n",
    "summary(GeneratorResNet(input_shape=(3, 256, 256), num_residual_blocks=9), input_size=((batch_size, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84eab0ab-05bf-49f6-b511-bd21e9736c3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummaryX\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGeneratorResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_residual_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: summary() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "summary(GeneratorResNet(input_shape=(3, 256, 256), num_residual_blocks=9), torch.zeros((1, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fecc6b5-f140-4572-894e-6586f470576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1          [-1, 3, 262, 262]               0\n",
      "            Conv2d-2         [-1, 64, 256, 256]           9,472\n",
      "    InstanceNorm2d-3         [-1, 64, 256, 256]               0\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5        [-1, 128, 128, 128]          73,856\n",
      "    InstanceNorm2d-6        [-1, 128, 128, 128]               0\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "            Conv2d-8          [-1, 256, 64, 64]         295,168\n",
      "    InstanceNorm2d-9          [-1, 256, 64, 64]               0\n",
      "             ReLU-10          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-11          [-1, 256, 66, 66]               0\n",
      "           Conv2d-12          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-13          [-1, 256, 64, 64]               0\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-15          [-1, 256, 66, 66]               0\n",
      "           Conv2d-16          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-17          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-18          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-19          [-1, 256, 66, 66]               0\n",
      "           Conv2d-20          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-21          [-1, 256, 64, 64]               0\n",
      "             ReLU-22          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-23          [-1, 256, 66, 66]               0\n",
      "           Conv2d-24          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-25          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-26          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-27          [-1, 256, 66, 66]               0\n",
      "           Conv2d-28          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-29          [-1, 256, 64, 64]               0\n",
      "             ReLU-30          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-31          [-1, 256, 66, 66]               0\n",
      "           Conv2d-32          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-33          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-34          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-35          [-1, 256, 66, 66]               0\n",
      "           Conv2d-36          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-37          [-1, 256, 64, 64]               0\n",
      "             ReLU-38          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-39          [-1, 256, 66, 66]               0\n",
      "           Conv2d-40          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-41          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-42          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-43          [-1, 256, 66, 66]               0\n",
      "           Conv2d-44          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-45          [-1, 256, 64, 64]               0\n",
      "             ReLU-46          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-47          [-1, 256, 66, 66]               0\n",
      "           Conv2d-48          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-49          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-50          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-51          [-1, 256, 66, 66]               0\n",
      "           Conv2d-52          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-53          [-1, 256, 64, 64]               0\n",
      "             ReLU-54          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-55          [-1, 256, 66, 66]               0\n",
      "           Conv2d-56          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-57          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-58          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-59          [-1, 256, 66, 66]               0\n",
      "           Conv2d-60          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-61          [-1, 256, 64, 64]               0\n",
      "             ReLU-62          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-63          [-1, 256, 66, 66]               0\n",
      "           Conv2d-64          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-65          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-66          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-67          [-1, 256, 66, 66]               0\n",
      "           Conv2d-68          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-69          [-1, 256, 64, 64]               0\n",
      "             ReLU-70          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-71          [-1, 256, 66, 66]               0\n",
      "           Conv2d-72          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-73          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-74          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-75          [-1, 256, 66, 66]               0\n",
      "           Conv2d-76          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-77          [-1, 256, 64, 64]               0\n",
      "             ReLU-78          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-79          [-1, 256, 66, 66]               0\n",
      "           Conv2d-80          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-81          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-82          [-1, 256, 64, 64]               0\n",
      "         Upsample-83        [-1, 256, 128, 128]               0\n",
      "           Conv2d-84        [-1, 128, 128, 128]         295,040\n",
      "   InstanceNorm2d-85        [-1, 128, 128, 128]               0\n",
      "             ReLU-86        [-1, 128, 128, 128]               0\n",
      "         Upsample-87        [-1, 128, 256, 256]               0\n",
      "           Conv2d-88         [-1, 64, 256, 256]          73,792\n",
      "   InstanceNorm2d-89         [-1, 64, 256, 256]               0\n",
      "             ReLU-90         [-1, 64, 256, 256]               0\n",
      "  ReflectionPad2d-91         [-1, 64, 262, 262]               0\n",
      "           Conv2d-92          [-1, 3, 256, 256]           9,411\n",
      "             Tanh-93          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 11,378,179\n",
      "Trainable params: 11,378,179\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1031.23\n",
      "Params size (MB): 43.40\n",
      "Estimated Total Size (MB): 1075.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(GeneratorResNet(input_shape=(3, 256, 256), num_residual_blocks=9), input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ec5e565-e876-4934-a1e1-44c1d6a69de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "         Layer (type)            Input Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "    ReflectionPad2d-1       [1, 3, 256, 256]               0               0\n",
      "             Conv2d-2       [1, 3, 262, 262]           9,472           9,472\n",
      "     InstanceNorm2d-3      [1, 64, 256, 256]               0               0\n",
      "               ReLU-4      [1, 64, 256, 256]               0               0\n",
      "             Conv2d-5      [1, 64, 256, 256]          73,856          73,856\n",
      "     InstanceNorm2d-6     [1, 128, 128, 128]               0               0\n",
      "               ReLU-7     [1, 128, 128, 128]               0               0\n",
      "             Conv2d-8     [1, 128, 128, 128]         295,168         295,168\n",
      "     InstanceNorm2d-9       [1, 256, 64, 64]               0               0\n",
      "              ReLU-10       [1, 256, 64, 64]               0               0\n",
      "     ResidualBlock-11       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-12       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-13       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-14       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-15       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-16       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-17       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-18       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "     ResidualBlock-19       [1, 256, 64, 64]       1,180,160       1,180,160\n",
      "          Upsample-20       [1, 256, 64, 64]               0               0\n",
      "            Conv2d-21     [1, 256, 128, 128]         295,040         295,040\n",
      "    InstanceNorm2d-22     [1, 128, 128, 128]               0               0\n",
      "              ReLU-23     [1, 128, 128, 128]               0               0\n",
      "          Upsample-24     [1, 128, 128, 128]               0               0\n",
      "            Conv2d-25     [1, 128, 256, 256]          73,792          73,792\n",
      "    InstanceNorm2d-26      [1, 64, 256, 256]               0               0\n",
      "              ReLU-27      [1, 64, 256, 256]               0               0\n",
      "   ReflectionPad2d-28      [1, 64, 256, 256]               0               0\n",
      "            Conv2d-29      [1, 64, 262, 262]           9,411           9,411\n",
      "              Tanh-30       [1, 3, 256, 256]               0               0\n",
      "=============================================================================\n",
      "Total params: 11,378,179\n",
      "Trainable params: 11,378,179\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pytorch_model_summary\n",
    "import torch\n",
    "print(pytorch_model_summary.summary(GeneratorResNet(input_shape=(3, 256, 256), num_residual_blocks=9), \n",
    "                                    torch.zeros(1, 3, 256, 256), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e9c84d4-931c-42e8-8c01-6fea73f731f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorResNet(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (11): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (12): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (13): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (14): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (15): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (16): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (20): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (24): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (28): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (29): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(G_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b12d1a-9284-4f1b-b6aa-79b6591b0c74",
   "metadata": {},
   "source": [
    "#### 5) GPU에 로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3512c-1b4f-47c4-be87-e49933552f89",
   "metadata": {},
   "source": [
    "- `torch.cuda.is_available()` : GPU 연산이 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aeb436b7-f374-45af-b891-192b9f741567",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda :\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3096f0-06a2-45a1-b950-d57cf73fd58b",
   "metadata": {},
   "source": [
    "#### 6) 가중치 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23246ad5-e613-41b8-a4d6-a815c5005154",
   "metadata": {},
   "source": [
    "- 앞서 선언한 weights_init_normal 함수로 Generator와 Discriminator의 가중치를 초기화한다.\n",
    "- `apply` 함수로 각 네트워크에 있는 모든 layer에 가중치 초기화를 적용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f8f5e6a-db45-41c6-9dbe-4a20cdac2760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): ZeroPad2d((1, 0, 1, 0))\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa9019-1bdb-4bd2-a7fc-52f8f67c8df2",
   "metadata": {},
   "source": [
    "#### 7) 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af391656-8c96-4c6c-8c51-d54207cfca11",
   "metadata": {},
   "source": [
    "- Generator와 Discriminator는 Adam 사용\n",
    "- `itertools.chain` : Optimizer가 여러 모델의 파라미터를 하나의 모델을 다루는 것처럼 동작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19bb7969-5202-44d8-a272-39331b043ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()),\n",
    "    lr = lr,\n",
    "    betas = (b1, b2)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d507fd1-aa36-4fb6-b6bd-0bd8e489983a",
   "metadata": {},
   "source": [
    "#### 8) 학습 스케쥴러"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6de117-a496-468b-89b7-6b1b9147bccc",
   "metadata": {},
   "source": [
    "- LambdaLR 클래스 정의\n",
    "    - Learning Rate를 Decay할 Epoch을 정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22c8669b-e126-407f-a78c-440a2443cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR :\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch) :\n",
    "        assert (n_epochs - decay_start_epoch) > 0 , \"Decay must start before the training session ends !\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "        \n",
    "    def step(self, epoch) :\n",
    "        return 1.0 - max(0, epoch+self.offset-self.decay_start_epoch) / (self.n_epochs-self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9aa67-4970-4c15-a0c0-6030732aa70f",
   "metadata": {},
   "source": [
    "- Learning Rate Scheduler 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c10eae9-9e9d-46af-aca0-53ec354476fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step)\n",
    "\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step)\n",
    "\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf266011-0fdf-43c9-a423-6e76fcbc3d27",
   "metadata": {},
   "source": [
    "- Tensor 연산에 사용할 Tensor 자료형 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0e28bd2-f893-4dc2-aca1-be8cae411d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ee2fa-561b-4f68-8b2c-f43a68ebb6ce",
   "metadata": {},
   "source": [
    "#### 9) ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68fbae-4803-4aee-a4f8-a438389895cc",
   "metadata": {},
   "source": [
    "- Torch 변수가 requires_grad = True로 지정되어 있다면 매 연산마다 Gradient를 저장하므로 CycleGAN의 학습을 위해서는 ReplayBuffer 클래스를 통해 이미지를 따로 저장해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "949b7690-b1f7-4938-91b1-706a6540578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer :\n",
    "    def __init__(self, max_size = 50) :\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "        \n",
    "    def push_and_pop(self, data) :\n",
    "        to_return = []\n",
    "        for element in data.data :\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) > self.max_size :\n",
    "                self.data.append(element)\n",
    "                to_return.append(elment)\n",
    "            else :\n",
    "                if random.uniform(0, 1) > 0.5 :\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else :\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8e43219-1f4b-4dd7-9fb6-83a06fbaf3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5017822-207b-4a5f-ae33-1422bc93ae75",
   "metadata": {},
   "source": [
    "#### 10) transform 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18d2f2-52f9-43e8-a0cf-016f3b35bb6e",
   "metadata": {},
   "source": [
    "- Dataset 클래스로 이미지를 불러오는 방식에 대하여 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10ea8191-e9b0-4168-ab0c-c24dd59f18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATIV\\AppData\\Local\\Temp\\ipykernel_11356\\3287249881.py:2: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
      "C:\\Users\\ATIV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e451e-82a1-4468-ae99-4d3a26f1602c",
   "metadata": {},
   "source": [
    "#### 11) DataLoader 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48a4ab-7349-4bf2-9c10-1b56a88e55bc",
   "metadata": {},
   "source": [
    "- 학습 혹은 평가 중 이미지 데이터를 불러올 DataLoader 정의\n",
    "- 앞서 정의한 Image Dataset 클래스로 selfie2anime 데이터 폴더로부터 transform_을 적용한 이미지를 배치 사이즈 만큼 불러온다.\n",
    "- num_workers로 cpu 유틸리티를 설정할 수 있다.\n",
    "- shuffle 변수를 True 혹은 False로 설정해 이미지를 무작위로 혹은 순차적으로 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30fa2e5e-93dd-4379-89ed-bc7f23664a2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training data loader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./datasets/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransforms_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munaligned\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_cpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Test data loader\u001b[39;00m\n\u001b[0;32m     10\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     11\u001b[0m     ImageDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./datasets/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_name, transforms_ \u001b[38;5;241m=\u001b[39m transforms_, unaligned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     12\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     13\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:277\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 277\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:97\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"./datasets/%s\" % dataset_name, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = n_cpu,\n",
    ")\n",
    "\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"./datasets/%s\" % dataset_name, transforms_ = transforms_, unaligned = True, mode=\"test\"),\n",
    "    batch_size = 5,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397d41b-9f8b-40fb-8232-5a92e653c194",
   "metadata": {},
   "source": [
    "#### 12) 생성한 샘플 이미지를 시각화하여 저장하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77840d65-7b6c-4d9d-b5d5-35d308b02bb8",
   "metadata": {},
   "source": [
    "- val_dataloader의 배치 사이즈가 5이므로 sample_images는 `make_grid` 함수를 통해 5개씩 샘플을 생성하여 images 폴더에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c59a983c-2258-4aa4-99ef-5cda47668754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done) :\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "    \n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    \n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34e754-0483-40f6-9ffd-71918ec1ea89",
   "metadata": {},
   "source": [
    "#### 13) 모델 학습 파이프라인 및 최종 학습 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55abe1e-f146-4a45-acd2-e6f17ac2d58c",
   "metadata": {},
   "source": [
    "- (1) Set model input\n",
    "    - dataloader에서 실제 사진 이미지(A)와 애니메이션 이미지(B)를 배치 사이즈만큼 불러온다.\n",
    "- (2) Adversarial ground truths\n",
    "    - Discriminator의 레이블 값을 만든다.\n",
    "    - valid와 fake 변수를 만든다.\n",
    "    - PatchGAN에 따라 valid는 Discriminator의 출력 크기만큼 전부 1로 채워지고, fake는 Discriminator의 출력 크기만큼 전부 0으로 채워진다.\n",
    "- (3) Train Generators\n",
    "    - Generator G_AB, G_BA를 학습 모드로 전환\n",
    "    - optimizer_G를 zero_grad()\n",
    "- (4) Identity Loss\n",
    "    - 색감, 형태 등을 유지하기 위한 Identity loss 계산\n",
    "    - G_BA에 실제 이미지 A를 입력한 후, 이를 real_A와 비교하여 L1Loss 계산\n",
    "    - G_AB도 마찬가지\n",
    "- (5) GAN Loss\n",
    "    - 이미지 real_A로부터 스타일이 변환된 가짜 이미지 fake_B를 생성\n",
    "    - D_B : 생성된 fake_B가 진짜인지 가짜인지 분류한다.\n",
    "    - D_B를 속이도록 G_AB의 GAN loss를 계산한다.\n",
    "    - real_B도 마찬가지 과정을 거쳐서 G_BA의 GAN loss 계산\n",
    "- (6) Cycle Loss\n",
    "    - G_BA로 가짜 이미지 fake_B에서 새로운 가짜 이미지 recov_A를 생성\n",
    "    - 이를 다시 원래 실제 이미지 real_A와 비교하여 L1 loss 계산\n",
    "- (7) Total Loss\n",
    "    - 앞서 계산한 loss를 총합하여 Generator의 전체 손실 함수를 계산하고 Generator의 가중치 업데이트\n",
    "    \n",
    "- (8) Train Discriminator A\n",
    "    - D_A의 가중치를 업데이트하는 과정\n",
    "    - 실제 이미지 real_A는 valid로 분류하고, fake_A는 fake로 분류\n",
    "    - 학습을 위해 optimizer_D_A를 zero_grad() 한다.\n",
    "- (9) Real Loss\n",
    "    - D_A가 진짜라고 판별한 경우, 실제 이미지 real_A의 MSE 손실함수를 계산한다.\n",
    "- (10) Fake Loss (on batch of previously generated samples)\n",
    "    - D_A가 가짜라고 판별한 경우, 가짜 이미지 fake_A_의 MSE 손실함수를 계산한다.\n",
    "    - fake_A의 가중치는 업데이트 가능한 상태이므로 detach() 함수로 값만 복사해온다.\n",
    "- (11) Total Loss\n",
    "    - (9), (10)에서 계산한 손실함수를 모두 합한 후 D_A의 가중치를 업데이트한다. \n",
    "    \n",
    "- (12) Train Discriminator B\n",
    "    - D_B의 가중치를 업데이트하는 과정도 D_A의 가중치를 업데이트 하는 것과 동일하다.\n",
    "    - D_B는 실제 이미지 real_B를 valid로 분류하고, fake_B는 fake로 분류한다.\n",
    "- (13) Real Loss\n",
    "    - D_B가 진짜라고 판별한 경우, 실제 이미지 real_B의 MSE 손실함수를 계산한다. \n",
    "- (14) Fake loss (on batch of previously generated samples)\n",
    "    - D_B가 가짜라고 판별한 경우, 가짜 이미지 fake_B의 MSE 손실함수를 계산한다.\n",
    "- (15) Total Loss\n",
    "    - (13), (14)에서 계산한 손실함수를 모두 합한 후 D_B의 가중치를 업데이트\n",
    "\n",
    "- (16) Determine approximate time left\n",
    "    - Epoch와 batch size로 남은 시간을 출력하기 위한 코드\n",
    "- (17) Print log\n",
    "    - Epoch와 batch size로 남은 시간을 출력하기 위한 코드\n",
    "- (18) If at sample interval save image\n",
    "    - 특정 epoch 간격마다 샘플로 생성한 이미지를 저장\n",
    "- (19) Update learning rates\n",
    "    - Generator와 Discriminator의 Learning Rate 스케쥴러를 업데이트\n",
    "- (20) Save model checkpoints\n",
    "    - 모델 가중치 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83d52446-b524-415f-9733-a7ca8e053d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'type' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m prev_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(init_epoch, n_epochs) :\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader) :\n\u001b[0;32m      4\u001b[0m         \n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# (1) Set model input\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         real_A \u001b[38;5;241m=\u001b[39m Variable(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(Tensor))\n\u001b[0;32m      7\u001b[0m         real_B \u001b[38;5;241m=\u001b[39m Variable(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(Tensor))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:569\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:226\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[List[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    225\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[0;32m    227\u001b[0m         batch\u001b[38;5;241m.\u001b[39mappend(idx)\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:66\u001b[0m, in \u001b[0;36mSequentialSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'type' has no len()"
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "for epoch in range(init_epoch, n_epochs) :\n",
    "    for i, batch in enumerate(dataloader) :\n",
    "        \n",
    "        # (1) Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "        \n",
    "        # (2) Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))),\n",
    "                         requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))),\n",
    "                        requires_grad = False)\n",
    "        \n",
    "        # (3) Train Generators\n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # (4) Identity Loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "        \n",
    "        # (5) GAN Loss\n",
    "        fake_B = G_AB(real_A)\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "        \n",
    "        # (6) Cycle Loss\n",
    "        recov_A = G_BA(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "        \n",
    "        # (7) Total Loss\n",
    "        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "        logg_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # (8) Train Discriminator A\n",
    "        optimizer_D_A.zero_grad()\n",
    "        \n",
    "            # (9) Real Loss\n",
    "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "        \n",
    "            # (10) Fake Loss (on batch of previously generated samples)\n",
    "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "        \n",
    "            # (11) Total Loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        \n",
    "        # (12) Train Discriminator B\n",
    "        optimizer_D_B.zero_grad()\n",
    "        \n",
    "            # (13) Real Loss\n",
    "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "        \n",
    "            # (14) Fake Loss (on batch of previously generated samples)\n",
    "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "        \n",
    "            # (15) Total Loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "        \n",
    "        # (16) Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds = batches_left * (time.time()-prev_time))\n",
    "        prev_time = time.time()\n",
    "        \n",
    "        # (17) Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss:%f, adv: %f, cycle: %f, identity: %f] ETA: %s\" % (\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_GAN.item(),\n",
    "                loss_cycle.item(),\n",
    "                loss_identity.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        # (18) If at sample interval save image\n",
    "        if batches_done % sample_interval == 0 :\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    # (19) Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "    \n",
    "    # (20) Save model checkpoints\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884797e-d6b9-4bcb-927e-cf486f7ae4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
