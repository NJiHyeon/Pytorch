{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42731fea-c8dc-400c-8bbf-c73429049ea9",
   "metadata": {},
   "source": [
    "### 03. 다중 선형 회귀(Multivariable Linear regression)\n",
    "앞서 배운 x가 1개인 선형 회귀를 단순 선형 회귀라고 한다. 이제는 다수의 x로부터 y를 예측하는 다중 선형 회귀에 대해서 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8bd7c-58f7-4630-a5b3-dfae7b2a3157",
   "metadata": {},
   "source": [
    "#### 1. 데이터에 대한 이해\n",
    "- 훈련 데이터 : 독립 변수 x의 개수가 3개로 3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델 만들기\n",
    "- 식 : H(x) = w1x1 + w2x2 + w3x3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddd388-a5f1-418c-913d-ef9dd76a8faa",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a01f1-2041-449a-bbd1-8f716085e463",
   "metadata": {},
   "source": [
    "#### 2. 파이토치로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a02c88a-fc9d-474e-964b-8b47f77882a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 도구들을 임포터하고 랜덤 시드 고정\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a1b35b-9774-4989-9384-7bac8c47da2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd808a30790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2206c9ac-80a6-43c8-9967-b81f81cf8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13811428-162d-48ff-ab3a-a05aa37218e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad = True)\n",
    "w2 = torch.zeros(1, requires_grad = True)\n",
    "w3 = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0110563-4133-410e-81ec-cafb816424a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1 : 0.718 w2 : 0.612 w3 : 0.680 b : 0.009 Cost : 1.078544\n",
      "Epoch  100/1000 w1 : 0.722 w2 : 0.608 w3 : 0.680 b : 0.009 Cost : 1.037795\n",
      "Epoch  200/1000 w1 : 0.727 w2 : 0.603 w3 : 0.681 b : 0.010 Cost : 0.999138\n",
      "Epoch  300/1000 w1 : 0.731 w2 : 0.599 w3 : 0.681 b : 0.010 Cost : 0.962497\n",
      "Epoch  400/1000 w1 : 0.735 w2 : 0.595 w3 : 0.681 b : 0.010 Cost : 0.927747\n",
      "Epoch  500/1000 w1 : 0.739 w2 : 0.590 w3 : 0.681 b : 0.010 Cost : 0.894806\n",
      "Epoch  600/1000 w1 : 0.743 w2 : 0.586 w3 : 0.682 b : 0.010 Cost : 0.863567\n",
      "Epoch  700/1000 w1 : 0.746 w2 : 0.582 w3 : 0.682 b : 0.010 Cost : 0.833937\n",
      "Epoch  800/1000 w1 : 0.750 w2 : 0.579 w3 : 0.682 b : 0.010 Cost : 0.805833\n",
      "Epoch  900/1000 w1 : 0.754 w2 : 0.575 w3 : 0.682 b : 0.010 Cost : 0.779167\n",
      "Epoch 1000/1000 w1 : 0.757 w2 : 0.571 w3 : 0.682 b : 0.011 Cost : 0.753894\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1) : \n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3 + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) **2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0 : \n",
    "        print('Epoch {:4d}/{} w1 : {:.3f} w2 : {:.3f} w3 : {:.3f} b : {:.3f} Cost : {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57d4d8-c255-4afd-b38c-5e481d974738",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b755e-a534-4169-b4cf-825b49285002",
   "metadata": {},
   "source": [
    "#### 3. 벡터와 행렬 연산으로 바꾸기\n",
    "- 위의 코드를 개선할 수 있는 부분 !\n",
    "    - x의 개수가 3개였으니까, x1_train, x2_train, x3_train, w1, w2, w3을 일일히 선언했다.\n",
    "    - 하지만 x의 개수가 1000개라면 ? -> 행렬 곱셈 연산(또는 벡터의 내적)을 사용\n",
    "- 행렬 곱셈 과정에서 이루어지는 벡터 연산을 백터의 내적이라고 한다.\n",
    "    - 가설을 벡터와 행렬 연산으로 표현할 수 있다 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a2afc-7351-42d7-a3c5-192186a82df7",
   "metadata": {},
   "source": [
    "##### 1. 벡터 연산으로 이해하기\n",
    "H(X) = w1x1 + w2x2 + w3x3\n",
    "- 위 식은 아래와 같이 두 벡터의 내적으로 표현할 수 있다.\n",
    "- 두 벡터를 각각 X와 W로 표현한다면, 가설은 : H(X) = XW\n",
    "- x의 개수가 3개였지만 이제는 X와 W라는 두 개의 변수로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d5ec1-f9db-4326-9626-9207175d8d23",
   "metadata": {},
   "source": [
    "##### 2. 행렬 연산으로 이해하기\n",
    "- 전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위를 샘플(sample)이라고 한다. (현재 샘플의 수는 총 5개)\n",
    "- 각 샘플에서 y를 결정하게 하는 각각의 독립 변수 x를 특성(feature)이라고 한다. (현재 특성은 3개)\n",
    "1. 이는 독립 변수 x들의 수가 15개임을 의미하고 독립변수 x들을 (샘플의 수 x 특성의 수)의 크기를 가지는 하나의 행렬로 표현 -> X\n",
    "2. 여기에 가중치 w1, w2, w3을 원소로 하는 벡터를 W라고 하고 이를 곱한다.\n",
    "3. 샘플 수만큼 차원을 가지는 편향 벡터 B를 만들어 더한다.\n",
    "- 위 식의 결과는 H(X) = XW + B로 전체 훈련 데이터의 가설 연산을 3개의 변수로만 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57176cd-71c4-4034-8e58-19ea4c0b5149",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129341e5-6a8a-4f8e-b103-f033747ffaa8",
   "metadata": {},
   "source": [
    "#### 4. 행렬 연산을 고려하여 파이토치로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "226cc354-4b92-404d-a064-7df8a71c6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 또한 행렬로 선언\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 80], \n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080c6916-ed28-4431-a17f-94165b0343cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbabdcf0-de26-483e-8443-35105e000d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 선언\n",
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8edcf5c7-f531-46fc-83d4-f86747720d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설을 행렬곱으로 간단히 정의\n",
    "hypothesis = x_train.matmul(W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb7c10a-e850-48c2-8a8c-6c9f36ce8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis : tensor([0., 0., 0., 0., 0.]) Cost : 29661.800781\n",
      "Epoch    1/20 hypothesis : tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost : 9537.694336\n",
      "Epoch    2/20 hypothesis : tensor([104.5421, 125.6208, 119.2478, 134.7861,  95.8280]) Cost : 3069.590820\n",
      "Epoch    3/20 hypothesis : tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost : 990.670288\n",
      "Epoch    4/20 hypothesis : tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost : 322.481964\n",
      "Epoch    5/20 hypothesis : tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost : 107.717064\n",
      "Epoch    6/20 hypothesis : tensor([148.9423, 178.9731, 169.8976, 192.0301, 136.5279]) Cost : 38.687401\n",
      "Epoch    7/20 hypothesis : tensor([151.1574, 181.6347, 172.4254, 194.8856, 138.5585]) Cost : 16.499046\n",
      "Epoch    8/20 hypothesis : tensor([152.4131, 183.1435, 173.8590, 196.5042, 139.7097]) Cost : 9.365656\n",
      "Epoch    9/20 hypothesis : tensor([153.1250, 183.9988, 174.6723, 197.4216, 140.3625]) Cost : 7.071105\n",
      "Epoch   10/20 hypothesis : tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost : 6.331867\n",
      "Epoch   11/20 hypothesis : tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost : 6.092532\n",
      "Epoch   12/20 hypothesis : tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0614]) Cost : 6.013823\n",
      "Epoch   13/20 hypothesis : tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost : 5.986775\n",
      "Epoch   14/20 hypothesis : tensor([154.0017, 185.0517, 175.6785, 198.5501, 141.1671]) Cost : 5.976314\n",
      "Epoch   15/20 hypothesis : tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost : 5.971213\n",
      "Epoch   16/20 hypothesis : tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost : 5.967797\n",
      "Epoch   17/20 hypothesis : tensor([154.0459, 185.1045, 175.7326, 198.6058, 141.2082]) Cost : 5.964961\n",
      "Epoch   18/20 hypothesis : tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost : 5.962292\n",
      "Epoch   19/20 hypothesis : tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost : 5.959693\n",
      "Epoch   20/20 hypothesis : tensor([154.0536, 185.1134, 175.7451, 198.6146, 141.2158]) Cost : 5.957091\n"
     ]
    }
   ],
   "source": [
    "# 비용함수와 옵티마이저를 정의하고 정해진 에포크만큼 훈련 진행\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 80], \n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1) :\n",
    "    \n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해진다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) **2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis : {} Cost : {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
